{
  "hash": "72c4c4958c4f17569e1fc764a247632b",
  "result": {
    "markdown": "---\ntitle: \"Anomaly Detection\"\nauthor: \"Aadyant Khatri\"\ndate: \"2023-12-05\"\ncategories: [outlier detection, code, analysis, ML]\nimage: \"image.jpg\"\n---\n\n\n# Anomaly Detection\nIn the realm of data analysis, anomalies and outliers serve as intriguing yet critical elements that deviate from the expected patterns within a dataset. Detecting these anomalies is pivotal in numerous fields, ranging from finance to healthcare, as they often signify important events, errors, or rare occurrences that demand attention and further exploration.\n\nAnomalies, also known as outliers, are data points or patterns that significantly differ from the majority of the dataset. They can manifest as sudden spikes, drops, or aberrations in the data and might represent errors, novelty, or genuinely unique occurrences.\n\nIdentifying anomalies is crucial for various reasons. In finance, they might signal fraudulent activities or market irregularities. In healthcare, anomalies could represent rare diseases or unexpected patient conditions. Furthermore, anomalies might indicate faults in machinery in industrial settings or outliers in consumer behavior in marketing.\n\n## Methods for Anomaly Detection\n\nThe dataset `mpg` from the `ggplot2` package will be used to illustrate the different approaches of outliers detection in R, and in particular we will focus on the variable hwy (highway miles per gallon).\n\n### Minimum and maximum\n\nThe initial stage in identifying outliers within R involves commencing with descriptive statistics, specifically focusing on the lowest and highest values, also known as the minimum and maximum.\n\nIn R, this can easily be done with the `summary()` function:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\ndat <- ggplot2::mpg\nsummary(dat$hwy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   18.00   24.00   23.44   27.00   44.00 \n```\n:::\n:::\n\nThis simple technique can readily identify obvious encoding errors, such as a human's weight listed as 786 kg (equivalent to 1733 pounds).\n\n### Histogram\nAn alternative fundamental method for identifying outliers involves creating a histogram representing the data.\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(dat$hwy,\n  xlab = \"hwy\",\n  main = \"Histogram of hwy\",\n  breaks = sqrt(nrow(dat))\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\nAccording to the histogram, there are a few observations that stand out as they are taller than the rest (notably the bar located on the right-hand side of the chart).\n\n### Boxplot\nBoxplots are great for spotting anomalies. They show data distribution and outliers well, helping identify observations that fall far outside the expected range. Anomalies often lie far beyond the \"whiskers\" of the boxplot, making it easy to visually detect them in a dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dat) +\n  aes(x = \"\", y = hwy) +\n  geom_boxplot(fill = \"blue\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\nA boxplot illustrates a quantitative variable by presenting five typical summary statistics (minimum, median, first and third quartiles, and maximum), along with any data points identified as possible outliers using the interquartile range (IQR) rule.\n\nData points identified as potential outliers based on the IQR rule are represented as individual points in the boxplot. According to this criterion, there are two potential outliers, which are visible as the two points positioned above the upper end of the boxplot.\n\nIt is also possible to extract the values of the potential outliers based on the IQR criterion thanks to the boxplot.stats()$out function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot.stats(dat$hwy)$out\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 44 44 41\n```\n:::\n:::\n\n\nWith the use of the `which()` function, it becomes feasible to retrieve the row number that corresponds to these exceptional values. Now armed with this information, you can readily revisit particular rows within the dataset to confirm them or display all variables related to these outliers.\n\n::: {.cell}\n\n```{.r .cell-code}\nout <- boxplot.stats(dat$hwy)$out\nout_ind <- which(dat$hwy %in% c(out))\ndat[out_ind, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 11\n  manufacturer model      displ  year   cyl trans  drv     cty   hwy fl    class\n  <chr>        <chr>      <dbl> <int> <int> <chr>  <chr> <int> <int> <chr> <chr>\n1 volkswagen   jetta        1.9  1999     4 manua~ f        33    44 d     comp~\n2 volkswagen   new beetle   1.9  1999     4 manua~ f        35    44 d     subc~\n3 volkswagen   new beetle   1.9  1999     4 auto(~ f        29    41 d     subc~\n```\n:::\n:::\n\n\n### Percentiles\n\nThis outlier detection technique relies on using percentiles. Employing this method involves identifying potential outliers by considering all data points lying outside the range established by the 2.5th and 97.5th percentiles.\n\nAlternatively, different percentile ranges, like the 1st and 99th, or the 5th and 95th, can also be utilized to create the range for outlier detection.\n\nTo compute the values for the lower and upper percentiles (and consequently establish the boundaries of the range), the `quantile()` function can be employed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlower_bound <- quantile(dat$hwy, 0.025)\nlower_bound\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.5% \n  14 \n```\n:::\n\n```{.r .cell-code}\nupper_bound <- quantile(dat$hwy, 0.975)\nupper_bound\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 97.5% \n35.175 \n```\n:::\n:::\n\n\nBased on this approach, any observations lower than 14 or higher than 35.175 will be regarded as potential outliers.\n\nAll variables considered as outliers with this method are as below:\n\n::: {.cell}\n\n```{.r .cell-code}\noutlier_ind <- which(dat$hwy < lower_bound | dat$hwy > upper_bound)\ndat[outlier_ind, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 11 x 11\n   manufacturer model    displ  year   cyl trans  drv     cty   hwy fl    class \n   <chr>        <chr>    <dbl> <int> <int> <chr>  <chr> <int> <int> <chr> <chr> \n 1 dodge        dakota ~   4.7  2008     8 auto(~ 4         9    12 e     pickup\n 2 dodge        durango~   4.7  2008     8 auto(~ 4         9    12 e     suv   \n 3 dodge        ram 150~   4.7  2008     8 auto(~ 4         9    12 e     pickup\n 4 dodge        ram 150~   4.7  2008     8 manua~ 4         9    12 e     pickup\n 5 honda        civic      1.8  2008     4 auto(~ f        25    36 r     subco~\n 6 honda        civic      1.8  2008     4 auto(~ f        24    36 c     subco~\n 7 jeep         grand c~   4.7  2008     8 auto(~ 4         9    12 e     suv   \n 8 toyota       corolla    1.8  2008     4 manua~ f        28    37 r     compa~\n 9 volkswagen   jetta      1.9  1999     4 manua~ f        33    44 d     compa~\n10 volkswagen   new bee~   1.9  1999     4 manua~ f        35    44 d     subco~\n11 volkswagen   new bee~   1.9  1999     4 auto(~ f        29    41 d     subco~\n```\n:::\n:::\n\nAccording to the percentiles technique, there are 11 possible anomalies.\n\nTo decrease this count, you have the option to adjust the percentiles to 1 and 99.\n\n::: {.cell}\n\n```{.r .cell-code}\nlower_bound <- quantile(dat$hwy, 0.01)\nupper_bound <- quantile(dat$hwy, 0.99)\n\noutlier_ind <- which(dat$hwy < lower_bound | dat$hwy > upper_bound)\n\ndat[outlier_ind, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 11\n  manufacturer model      displ  year   cyl trans  drv     cty   hwy fl    class\n  <chr>        <chr>      <dbl> <int> <int> <chr>  <chr> <int> <int> <chr> <chr>\n1 volkswagen   jetta        1.9  1999     4 manua~ f        33    44 d     comp~\n2 volkswagen   new beetle   1.9  1999     4 manua~ f        35    44 d     subc~\n3 volkswagen   new beetle   1.9  1999     4 auto(~ f        29    41 d     subc~\n```\n:::\n:::\n\n\n### Grubbs’s test\nThe Grubbs test identifies whether the extreme values in a dataset, either the highest or lowest, are outliers.\n\nThe Grubbs test examines outliers individually, focusing on one extreme value (either the highest or lowest) at a time. Therefore, the null hypothesis suggests that the highest value is not considered an outlier, while the alternative hypothesis proposes that the highest value is indeed an outlier.\n\nFor any statistical test, when the p-value falls below the selected significance threshold (usually α = 0.05), we reject the null hypothesis and infer that the lowest/highest value represents an outlier.\n\nConversely, if the p-value is equal to or greater than the significance level, we do not reject the null hypothesis. In this case, we conclude that, based on the data, there's insufficient evidence to reject the idea that the lowest/highest value is not an outlier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(outliers)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'outliers' was built under R version 4.1.3\n```\n:::\n\n```{.r .cell-code}\ntest <- grubbs.test(dat$hwy)\ntest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tGrubbs test for one outlier\n\ndata:  dat$hwy\nG = 3.45274, U = 0.94862, p-value = 0.05555\nalternative hypothesis: highest value 44 is an outlier\n```\n:::\n:::\n\nThe p-value of 0.056 suggests that, at a 5% significance level, we fail to reject the hypothesis indicating that the highest value of 44 might not be considered an outlier.\n\n### Dixon’s test\n\nLike the Grubb's test, the Dixon test is utilized to determine if a particular low or high value is an outlier. If there are suspicions of multiple outliers, this test needs to be conducted separately on each suspected outlier.\n\nThe Dixon test is particularly beneficial when dealing with limited sample sizes, typically when `n` is equal to or less than 25.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubdat <- dat[1:20, ]\ntest <- dixon.test(subdat$hwy)\ntest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tDixon test for outliers\n\ndata:  subdat$hwy\nQ = 0.57143, p-value = 0.006508\nalternative hypothesis: lowest value 15 is an outlier\n```\n:::\n:::\n\nThe findings indicate that the smallest figure, 15, stands out as an outlier (with a p-value of 0.007).\n\n### Machine Learning Approaches\n\nMachine learning algorithms offer a diverse toolkit for anomaly detection due to their ability to comprehend complex patterns within datasets. Several specific algorithms, such as clustering, isolation forests, one-class SVM (Support Vector Machine), and neural networks, showcase proficiency in identifying anomalies by learning from the inherent structures and characteristics of the data.\n\n#### Clustering Algorithms\nClustering algorithms, such as K-means or DBSCAN, group similar data points together based on certain similarity metrics. Anomalies are often isolated as data points that don’t fit into any distinct cluster or form clusters of their own, highlighting their dissimilarity from the majority of the data.\n\n#### One-Class SVM (Support Vector Machine)\nOne-class SVM is specifically designed for anomaly detection by learning the patterns of normal data instances. It constructs a boundary around the normal data points in a high-dimensional space, effectively identifying anomalies as instances lying outside this boundary.\n\n#### Neural Networks\nNeural networks, particularly deep learning models, possess the capability to detect anomalies by learning intricate data representations. Autoencoders, a type of neural network architecture, are commonly used for anomaly detection. They reconstruct input data and pinpoint anomalies by their inability to accurately reconstruct, highlighting deviations from learned patterns.\n\n## Conclusion\nAnomaly and outlier detection play a pivotal role in data analysis across diverse domains. By leveraging statistical, machine learning, and visualization techniques, analysts can uncover hidden insights, potential errors, or significant events within datasets.\n\nUnderstanding the context and domain-specific knowledge are essential in defining anomalies and selecting suitable detection methods. Robust anomaly detection not only aids in identifying issues but also reveals valuable insights that might lead to improved decision-making, anomaly prevention, and enhanced overall data quality. Mastering anomaly detection techniques empowers analysts to extract meaningful information and glean deeper insights from their data, fostering more informed actions across various industries and disciplines.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}