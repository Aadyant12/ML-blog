{
  "hash": "803795ce90a41c7698fe413cba44d9dd",
  "result": {
    "markdown": "---\ntitle: \"Probability Theory and Random Variables\"\nauthor: \"Aadyant Khatri\"\ndate: \"2023-12-05\"\ncategories: [probability, code, analysis, ML]\nimage: \"image.jpg\"\n---\n\n\n# Probability\n\nIn games involving chance, the concept of probability is easily understood. For instance, it's intuitive to grasp that the odds of rolling a seven with a pair of dice are 1 in 6. However, beyond gaming, the meaning of probability varies in different contexts. Today, probability theory has a much broader application, becoming a common term in everyday language. Google's auto-complete suggestions for \"What are the chances of\" include queries about having twins, rain, lightning strikes, and cancer.\n\nUnderstanding how to calculate probabilities provides an advantage in games of chance, leading many brilliant minds throughout history, including renowned mathematicians like Cardano, Fermat, and Pascal, to dedicate time and effort to unraveling the mathematics behind these games. This exploration gave birth to Probability Theory. Presently, probability remains incredibly relevant in contemporary games of chance, such as poker, where computing the probability of winning a hand based on visible cards is crucial. Casinos also heavily rely on probability theory to create games that almost certainly secure profits.\n\nHowever, probability theory extends far beyond gaming and is highly valuable in various contexts, especially those reliant on data influenced by chance in some manner.\n\n## Probability distributions\n\nWhen we have information about the occurrence rates of various categories, establishing a distribution for categorical outcomes becomes fairly simple. We merely allocate a probability to each category. For situations resembling objects in a container (like beads in an urn), the distribution is determined by the proportion of each type of object.\n\nFor instance, if we're randomly contacting potential voters from a population consisting of 44% Democrats, 44% Republicans, 10% undecided, and 2% Green Party members, these percentages serve as the probabilities for each group. Hence, the probability distribution is as follows:\n\n|                          |      |\n|--------------------------|------|\n| Pr(picking a Democrat)   | 0.44 |\n| Pr(picking a Republican) | 0.44 |\n| Pr(picking an undecided) | 0.10 |\n| Pr(picking a Green)      | 0.02 |\n\n## Monte Carlo simulations for categorical data\n\nComputers offer the capability to carry out the described simple random experiment: for instance, selecting a bead randomly from a bag holding three blue beads and two red ones. Random number generators allow us to simulate this random selection process.\n\nAn illustration of this is the sample function in R. To showcase its application, we display its use in the code provided below. Initially, we utilize the function \"rep\" to create the collection of beads, resembling an urn, and subsequently employ \"sample\" to randomly select a bead from this simulated urn.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeads <- rep(c(\"red\", \"blue\"), times = c(2,3))\nbeads\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\"\n```\n:::\n\n```{.r .cell-code}\nsample(beads, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"red\"\n```\n:::\n:::\n\n\nThis code generates a single random outcome. To simulate an infinite repetition of this experiment, which is practically impossible, we opt to repeat it a sufficiently large number of times, approximating an infinite scenario. **This approach is an illustration of a Monte Carlo simulation.**\n\nWe utilize the replicate function, enabling us to iterate the same action any desired number of times. In this instance, we repeat the random event X = 10,000 times. Subsequently, we verify if our defined definition aligns with the approximation derived from this Monte Carlo simulation. Here, `prop.table` gives us the proportions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- 10000\nevents <- replicate(X, sample(beads, 1))\ntab <- table(events)\nprop.table(tab)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nevents\n  blue    red \n0.5961 0.4039 \n```\n:::\n:::\n\n\nThe figures presented represent the estimated probabilities from this Monte Carlo simulation. According to statistical theory, as the value of 'X' increases, the approximations tend to converge towards 3/5, equivalent to 0.6, and 2/5, equivalent to 0.4.\n\n## Independence\n\nIn probability, events A and B are independent if the occurrence of one doesn't influence the probability of the other. Mathematically, this means that the probability of both events happening is the product of their individual probabilities. This concept simplifies calculations but doesn't imply the events are unrelated in reality.\n\nWhen you toss a fair coin and roll a fair six-sided die, these events are **independent**. The outcome of the coin toss (heads or tails) does not affect the outcome of rolling the die (getting a specific number). The probabilities of each event are not influenced by the other.\n\nDrawing cards from a deck without replacing them creates **dependent** events. For instance, after drawing a card from a standard deck without replacing it, the probability of drawing a specific card on the next draw changes because the deck's composition has altered.\n\n## Conditional probabilities\n\nWhen events are not independent, conditional probabilities are useful. We saw an example earlier where we talked about drawing two cards from a deck of cards without replacement. Consider:\n\n```         \nPr(Card 2 is a king âˆ£ Card 1 is a king ) = 3/51\n```\n\nWe use the ```|``` as shorthand for \"given that\".\n\nWhen two events, say are independent, we have:\n\n```         \nPr(A | B) = Pr(A)\n```\n\nIn mathematical terms, this means that the occurrence of event `B` has no bearing on the probability of event `A` happening.\n\n## Arithmetic rules\n### Addition rule\nThe addition rule tells us that:\n```\nPr(A or B) = Pr(A) + Pr(B) - Pr(A and B)\n```\n\nThis rule is easy to understand if you visualize a Venn diagram. When we add probabilities directly, we're counting the overlapping part twice, thus necessitating the subtraction of one instance to correct for the duplication.\n\n### Multiplication rule\nIf we seek to determine the likelihood of two events, for instance, A and B, happening simultaneously, we can apply the multiplication rule:\n```\nPr(A and B) = Pr(A) * Pr(B|A)\n```\nSuppose you have a bag with 4 red balls and 6 blue balls. You draw one ball, note its color, and without replacing it, draw another ball. Let's calculate the probability of drawing a red ball followed by drawing another red ball:\n\nFirst Event: Probability of drawing a red ball = Number of red balls / Total number of balls = 4 / 10 = 2/5.\n\nSecond Event: If the first ball drawn was red (which has happened with a probability of 2/5), now there are 3 red balls left out of 9 total balls. Therefore, the probability of drawing a second red ball after the first red ball was drawn = 3 / 9 = 1/3.\n\nNow, according to the multiplication rule, the overall probability of both events happening (drawing two red balls consecutively) is found by multiplying the probabilities of the individual events:\n\nProbability of drawing two red balls = Probability of the first red ball * Probability of the second red ball given the first was red\n= (2/5) * (1/3)\n= 2/15.\n\nSo, the probability of drawing two red balls consecutively, considering the dependency, is 2/15.\n\n## Example\nNow let's discuss one very prominent problems in probability theory: **Birthday problem**\n\nLet's consider a scenario in a classroom containing 50 individuals. If we assume these 50 people were selected randomly, what is the probability that at least two individuals share the same birthday? To determine this probability, we'll employ a Monte Carlo simulation. For the sake of simplicity, we disregard individuals born on February 29, as this adjustment doesn't significantly alter the outcome.\n\nInitially, consider that birthdays can be depicted as values ranging from 1 to 365. Therefore, a collection of 50 birthdays can be acquired in the following manner. To verify whether there are at least two individuals with the same birthday within this specific group of 50 people, we can utilize the `duplicated` function. This function identifies duplicates within a vector and returns `TRUE` if any element is a duplicate. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 50\nbdays <- sample(1:365, n, replace = TRUE)\nduplicated(c(1,2,3,1,4,3,5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\n```\n:::\n:::\n\n\nTo check if two birthdays match, we can easily employ the `any` and `duplicated` functions in this manner:\n\n::: {.cell}\n\n```{.r .cell-code}\nany(duplicated(bdays))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nTo estimate the probability of having a common birthday within a group, we conduct multiple trials by selecting groups of 50 birthdays repeatedly.\n\n::: {.cell}\n\n```{.r .cell-code}\nB <- 10000\nsame_birthday <- function(n){\n  bdays <- sample(1:365, n, replace=TRUE)\n  any(duplicated(bdays))\n}\nresults <- replicate(B, same_birthday(50))\nmean(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9697\n```\n:::\n:::\n\nWe can quickly create a function to compute this for any group size. By utilizing the `sapply` function, we can execute operations on individual elements using any specified function. Now, we're able to create a graph illustrating the estimated probabilities of two individuals sharing a birthday within a gathering of size `n`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompute_prob <- function(n, B=10000){\n  results <- replicate(B, same_birthday(n))\n  mean(results)\n}\n\nn <- seq(1,60)\nprob <- sapply(n, compute_prob)\n\nprob <- sapply(n, compute_prob)\nqplot(n, prob)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nLet's calculate precise probabilities instead of relying on Monte Carlo estimations. By employing mathematical methods, we obtain accurate solutions, and the calculations are significantly quicker because we're not required to conduct experiments to generate results.\n\nFirst, considering the initial person, the likelihood of them having a distinct birthday is 1. Following this, the probability that the second person possesses a unique birthday, given that the first person has already selected one, is 364 out of 365. Subsequently, provided the first two individuals have distinct birthdays, the third person is left with 363 days to choose from. Continuing this pattern, the probability of all 50 people having different birthdays is then determined. Then the multiplication rule can be applied to find the probability of having a clash of birthdays.\n\nThe code can be written for `n` number of people:\n\n::: {.cell}\n\n```{.r .cell-code}\nexact_prob <- function(n){\n  prob_unique <- seq(365,365-n+1)/365 \n  1 - prod( prob_unique)\n}\nexact_probs <- sapply(n, exact_prob)\nqplot(n, prob) + geom_line(aes(n, exact_probs), col = \"green\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThis graph demonstrates that the Monte Carlo simulation offered a highly accurate estimation of the precise probability.\n\n# Random Variables\n\nRandom variables are numerical results arising from unpredictable procedures. Generating random variables can be straightforward using illustrations like those we've demonstrated. For instance, consider defining X as 1 when a bead is blue and 0 otherwise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeads <- rep( c(\"red\", \"blue\"), times = c(2,3))\nX <- ifelse(sample(beads, 1) == \"blue\", 1, 0)\n```\n:::\n\n\nIn this context, X represents a random variable where the outcome alters randomly each time we pick a new bead. See the example provided below:\n\n::: {.cell}\n\n```{.r .cell-code}\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\nifelse(sample(beads, 1) == \"blue\", 1, 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\n## Probability distribution of a random variable\n\nThe probability distribution of a random variable describes the likelihood of different outcomes or values that the variable can take on, along with their associated probabilities. It provides a comprehensive overview of the probabilities associated with each possible value of the random variable. More specifically, the probability distribution of a random variable, say S, tells us the probability of the observed value falling at any given interval.\n\nIf we can define a cumulative distribution function $F(a) = Pr(S \\le a)$, then we will be able to answer any question related to the probability of events defined by our random variable `S`. We call this `F` the random variableâ€™s distribution function.\n\nWe can approximate the distribution function of the random variable by employing a Monte Carlo simulation, generating numerous instances of the random variable.\n\n## Expected value and standard error\nIt is common to use letter `E` like this:\n$$\nE[X]\n$$\nto denote the expected value of the random variable `X`.\n\nA random variable fluctuates around its expected value, and with numerous iterations, the average of these iterations converges toward the expected value, becoming more precise with increased iterations.\n\nTheoretical statistics offers methods to compute expected values in various scenarios. For instance, a valuable formula indicates that the expected value of a random variable based on a single draw is the average of the numbers within the container or set.\n\nIn general, if an urn has two possible outcomes, say `a` and `b`, with proportions `p` and `(1-p)` respectively, the average is:\n$$\nE[X] = ap + b(1-p)\n$$\n\nThe **standard error** (SE) provides insight into the extent of variability around the expected value. In statistical literature, it's typical to represent the standard error of a random variable as \n$$\nSE[X]\n$$\n\nIn general, if an urn has two possible outcomes, say `a` and `b`, with proportions `p` and `(1-p)` respectively, the average is:\n$$\nSE[X] = |b-a|\\sqrt{p(1-p)}\n$$\n## Central Limit Theorem\n\nThe Central Limit Theorem (CLT) states that when the number of draws, known as the sample size, is sufficiently large, the probability distribution of the sum of independent draws tends to be close to a normal distribution. As sampling models are utilized across numerous data generation procedures, the CLT is widely regarded as one of the most significant and influential mathematical concepts in history.\n\nCLT operates effectively when the number of samples is considerable, but what qualifies as \"large\" is subjective. In various cases, as few as 30 samples might suffice for the CLT to be applicable, and in specific situations, even 10 samples could be adequate. It's important to note that these are not universal guidelines. It's worth considering that when the likelihood of success is extremely low, larger sample sizes become necessary.\n\n## Law of large numbers\n\nAs the sample size (n) increases, the standard error of the mean decreases. When n becomes sufficiently large, the standard error approaches zero, and the average of the samples tends to match the average of the entire population. This phenomenon is referred to in statistical literature as the law of large numbers or the law of averages.\n\nThe misconception surrounding the law of averages often leads to misinterpretations. For instance, if you flip a coin five times and observe heads each time, someone might mistakenly argue that the next toss is likely to result in tails because, on average, there should be a 50-50 split between heads and tails. Similarly, there's a tendency to anticipate a red outcome on a roulette wheel after witnessing five consecutive black spins. However, these events are independent, so the chance of a coin landing heads remains 50%, regardless of the previous outcomes. This principle also applies to roulette. The law of averages is relevant only when the number of trials is significantly large, not in small samples. For instance, after a million coin tosses, you're expected to see approximately 50% heads, irrespective of the initial five tosses.\n\nAnother funny misapplication of the law of averages occurs in sports when television sportscasters predict a player's imminent success just because they've encountered a few failures in a row.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}