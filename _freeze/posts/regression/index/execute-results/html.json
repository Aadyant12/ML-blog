{
  "hash": "051a007b4551f89a7127e2d3a3eefc31",
  "result": {
    "markdown": "---\ntitle: \"Regression\"\nauthor: \"Aadyant Khatri\"\ndate: \"2023-12-05\"\ncategories: [regression, code, analysis, ML]\nimage: \"image.jpeg\"\n---\n\n\n# Linear Regression\n\nLinear regression stands as a fundamental concept in statistics and machine learning, serving as a cornerstone for predictive modeling. Its simplicity and effectiveness make it a widely used technique in various fields, from finance to healthcare and beyond. In this blog post, we'll delve into the basics of linear regression, its applications, and how it works.\n\nAt its core, linear regression is a statistical method used to understand the relationship between two variables: an independent variable (predictor) and a dependent variable (outcome). The goal is to establish a linear relationship between these variables, enabling predictions and understanding the impact of changes in the predictor on the outcome.\n\nSimple linear regression involves one independent variable, while multiple linear regression deals with several predictors. Both aim to fit a linear equation to the data that best explains the relationship between variables. The equation takes the form: $Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\epsilon$, where $Y$ is the dependent variable, $X$s represent the predictors, $\\beta$s are coefficients, and $\\epsilon$ is the error term.\n\nTo understand regression thoroughly we will use a data-driven approach that examines the height data from families to try to understand heredity.\n\nIf we were to specify a linear model for this data, we would denote the $N$ observed father heights with $x_1, x_2, ....x_n$, then we model the $N$ son heights we are trying to predict with:\n$$Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i, i = 1, 2, ....N.$$\nwhere $Y_i$ is the random son’s height that we want to predict.\n\nOne reason why linear models are popular is their interpretability. Regarding the above data, we can explain the data as follows: due to inherited genes, a son's predicted height increases by $\\beta_1$ for each additional inch in the father’s height, denoted by $x$. However, not all sons with fathers of the same height $x$ have identical heights. This disparity is accounted for by the term $\\epsilon$, which represents the remaining variability. This variability encompasses factors like the mother’s genetic influence, environmental aspects, and other biological random variations.\n\nThe way we formulated the model indicates that the intercept $\\beta_0$ lacks interpretability because it represents the predicted height of a son with a father having zero height, which is not practically meaningful in this context.\n\nIn order for linear models to be effective, it's necessary to estimate the unknown values of $\\beta$. The conventional scientific approach involves determining the values that minimize the difference between the model and the actual data. This process is known as the least squares (LS) equation, a concept that will be frequently encountered in this chapter. For our data, the equation can be represented as:\n\n$$RSS = \\sum_{i=1}^{n} (y_i - (β_0 + β_1x_1))^2$$\n\nThis calculation represents the residual sum of squares (RSS). Once we identify the values that minimize the RSS, these values are referred to as the least squares estimates (LSE).\n\nWithin the R programming language, the least squares estimates can be acquired by utilizing the `lm` function.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"GaltonFamilies\")\nset.seed(1983)\ngalton_heights <- GaltonFamilies |>\n  filter(gender == \"male\") |>\n  group_by(family) |>\n  sample_n(1) |>\n  ungroup() |>\n  select(father, childHeight) |>\n  rename(son = childHeight)\nfit <- lm(son ~ father, data = galton_heights)\nfit$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)      father \n  37.287605    0.461392 \n```\n:::\n:::\n\nOn visualizing the heights of the fathers and their sons as well as the best linear fitting model, we can say that linear regressor has worked fine.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngalton_heights |> ggplot(aes(son, father)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n# Non - Linear Regression\n\nTraditional linear regression assumes a linear relationship between the independent and dependent variables, following a straight line or plane. However, real-world phenomena often exhibit more complex and curvilinear patterns. Non-linear regression, unlike its linear counterpart, allows for fitting models to non-linear data by employing curved lines, surfaces, or more intricate functions.\n\nUnderstanding when to opt for non-linear regression is pivotal. Many natural processes, such as biological, economic, or physical systems, display patterns that cannot be adequately represented by linear models. For instance, growth patterns, saturation effects, or exponential decay are common scenarios where non-linear regression becomes indispensable.\n\nNon-linear regression encompasses a wide spectrum of models. These include polynomial, exponential, logarithmic, power-law, sigmoidal, and many other complex functions. Each model is tailored to suit specific data behaviors, aiming to accurately capture and predict relationships that linear models cannot effectively represent.\n\nFor example, let's create a custom data where \n$$x = 1, 2, ....50$$\nand\n$$ y = x^3 - 2x^2 + x + 2 + \\epsilon$$\nwhere $\\epsilon$ denotes random noise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1:50\ny <- x^3 - 2 * x^2 + x + 2 + rnorm(10, 0, 10)\ndf <- data.frame(x, y)\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    x             y\n1   1  5.323605e-01\n2   2  3.468994e+00\n3   3 -7.013884e+00\n4   4  3.167791e+01\n5   5  9.397093e+01\n6   6  1.413180e+02\n7   7  2.533070e+02\n8   8  3.813409e+02\n9   9  5.573736e+02\n10 10  7.862314e+02\n11 11  1.100532e+03\n12 12  1.453469e+03\n13 13  1.852986e+03\n14 14  2.361678e+03\n15 15  2.953971e+03\n16 16  3.591318e+03\n17 17  4.353307e+03\n18 18  5.191341e+03\n19 19  6.137374e+03\n20 20  7.196231e+03\n21 21  8.400532e+03\n22 22  9.703469e+03\n23 23  1.111299e+04\n24 24  1.269168e+04\n25 25  1.441397e+04\n26 26  1.624132e+04\n27 27  1.825331e+04\n28 28  2.040134e+04\n29 29  2.271737e+04\n30 30  2.520623e+04\n31 31  2.790053e+04\n32 32  3.075347e+04\n33 33  3.377299e+04\n34 34  3.702168e+04\n35 35  4.047397e+04\n36 36  4.409132e+04\n37 37  4.795331e+04\n38 38  5.201134e+04\n39 39  5.629737e+04\n40 40  6.081623e+04\n41 41  6.560053e+04\n42 42  7.060347e+04\n43 43  7.583299e+04\n44 44  8.135168e+04\n45 45  8.713397e+04\n46 46  9.314132e+04\n47 47  9.945331e+04\n48 48  1.060213e+05\n49 49  1.128774e+05\n50 50  1.200262e+05\n```\n:::\n:::\n\nOn fitting both polynomial (degree = 3) and linear models we can see that the former performs much better. \n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(y ~ x, data = df)\ndf |> ggplot(aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x) + \n  geom_smooth(method = 'loess', formula = y~poly(x, 3))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/setup-1.png){width=672}\n:::\n:::\n\n\n# Conclusion\n\nIn summary, linear regression excels in capturing straightforward relationships between variables, offering simplicity and interpretability. Meanwhile, non-linear regression shines when dealing with more complex, non-linear data patterns that linear models cannot effectively represent. \n\nThe selection between linear and non-linear regression hinges on the nature of the data and the complexity of the relationship between variables. While linear regression provides a clear interpretation of linear trends, non-linear regression accommodates more nuanced, intricate relationships.\n\nBoth techniques possess their unique strengths and limitations. Linear regression is straightforward and computationally less demanding, whereas non-linear regression offers flexibility at the cost of potentially higher computational resources and the need to guard against overfitting.\n\nA comprehensive understanding of both linear and non-linear regression equips analysts and researchers with a versatile toolkit to effectively model relationships within diverse datasets. This proficiency aids in making accurate predictions and informed decisions across various domains, optimizing analyses for specific data complexities and objectives.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}