[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML blog",
    "section": "",
    "text": "Anomaly Detection\n\n\n\n\n\n\n\noutlier detection\n\n\ncode\n\n\nanalysis\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nAadyant Khatri\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nclassification\n\n\ncode\n\n\nanalysis\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nAadyant Khatri\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nclustering\n\n\ncode\n\n\nanalysis\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nAadyant Khatri\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\nprobability\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nAadyant Khatri\n\n\n\n\n\n\n  \n\n\n\n\nRegression\n\n\n\n\n\n\n\nregression\n\n\ncode\n\n\nanalysis\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nAadyant Khatri\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Anomaly detection/index.html",
    "href": "posts/Anomaly detection/index.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "In the realm of data analysis, anomalies and outliers serve as intriguing yet critical elements that deviate from the expected patterns within a dataset. Detecting these anomalies is pivotal in numerous fields, ranging from finance to healthcare, as they often signify important events, errors, or rare occurrences that demand attention and further exploration.\nAnomalies, also known as outliers, are data points or patterns that significantly differ from the majority of the dataset. They can manifest as sudden spikes, drops, or aberrations in the data and might represent errors, novelty, or genuinely unique occurrences.\nIdentifying anomalies is crucial for various reasons. In finance, they might signal fraudulent activities or market irregularities. In healthcare, anomalies could represent rare diseases or unexpected patient conditions. Furthermore, anomalies might indicate faults in machinery in industrial settings or outliers in consumer behavior in marketing.\n\n\nThe dataset mpg from the ggplot2 package will be used to illustrate the different approaches of outliers detection in R, and in particular we will focus on the variable hwy (highway miles per gallon).\n\n\nThe initial stage in identifying outliers within R involves commencing with descriptive statistics, specifically focusing on the lowest and highest values, also known as the minimum and maximum.\nIn R, this can easily be done with the summary() function:\n\nlibrary(ggplot2)\ndat &lt;- ggplot2::mpg\nsummary(dat$hwy)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   18.00   24.00   23.44   27.00   44.00 \n\n\nThis simple technique can readily identify obvious encoding errors, such as a human’s weight listed as 786 kg (equivalent to 1733 pounds).\n\n\n\nAn alternative fundamental method for identifying outliers involves creating a histogram representing the data.\n\nhist(dat$hwy,\n  xlab = \"hwy\",\n  main = \"Histogram of hwy\",\n  breaks = sqrt(nrow(dat))\n)\n\n\n\n\nAccording to the histogram, there are a few observations that stand out as they are taller than the rest (notably the bar located on the right-hand side of the chart).\n\n\n\nBoxplots are great for spotting anomalies. They show data distribution and outliers well, helping identify observations that fall far outside the expected range. Anomalies often lie far beyond the “whiskers” of the boxplot, making it easy to visually detect them in a dataset.\n\nggplot(dat) +\n  aes(x = \"\", y = hwy) +\n  geom_boxplot(fill = \"blue\") +\n  theme_minimal()\n\n\n\n\nA boxplot illustrates a quantitative variable by presenting five typical summary statistics (minimum, median, first and third quartiles, and maximum), along with any data points identified as possible outliers using the interquartile range (IQR) rule.\nData points identified as potential outliers based on the IQR rule are represented as individual points in the boxplot. According to this criterion, there are two potential outliers, which are visible as the two points positioned above the upper end of the boxplot.\nIt is also possible to extract the values of the potential outliers based on the IQR criterion thanks to the boxplot.stats()$out function:\n\nboxplot.stats(dat$hwy)$out\n\n[1] 44 44 41\n\n\nWith the use of the which() function, it becomes feasible to retrieve the row number that corresponds to these exceptional values. Now armed with this information, you can readily revisit particular rows within the dataset to confirm them or display all variables related to these outliers.\n\nout &lt;- boxplot.stats(dat$hwy)$out\nout_ind &lt;- which(dat$hwy %in% c(out))\ndat[out_ind, ]\n\n# A tibble: 3 x 11\n  manufacturer model      displ  year   cyl trans  drv     cty   hwy fl    class\n  &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1 volkswagen   jetta        1.9  1999     4 manua~ f        33    44 d     comp~\n2 volkswagen   new beetle   1.9  1999     4 manua~ f        35    44 d     subc~\n3 volkswagen   new beetle   1.9  1999     4 auto(~ f        29    41 d     subc~\n\n\n\n\n\nThis outlier detection technique relies on using percentiles. Employing this method involves identifying potential outliers by considering all data points lying outside the range established by the 2.5th and 97.5th percentiles.\nAlternatively, different percentile ranges, like the 1st and 99th, or the 5th and 95th, can also be utilized to create the range for outlier detection.\nTo compute the values for the lower and upper percentiles (and consequently establish the boundaries of the range), the quantile() function can be employed.\n\nlower_bound &lt;- quantile(dat$hwy, 0.025)\nlower_bound\n\n2.5% \n  14 \n\nupper_bound &lt;- quantile(dat$hwy, 0.975)\nupper_bound\n\n 97.5% \n35.175 \n\n\nBased on this approach, any observations lower than 14 or higher than 35.175 will be regarded as potential outliers.\nAll variables considered as outliers with this method are as below:\n\noutlier_ind &lt;- which(dat$hwy &lt; lower_bound | dat$hwy &gt; upper_bound)\ndat[outlier_ind, ]\n\n# A tibble: 11 x 11\n   manufacturer model    displ  year   cyl trans  drv     cty   hwy fl    class \n   &lt;chr&gt;        &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n 1 dodge        dakota ~   4.7  2008     8 auto(~ 4         9    12 e     pickup\n 2 dodge        durango~   4.7  2008     8 auto(~ 4         9    12 e     suv   \n 3 dodge        ram 150~   4.7  2008     8 auto(~ 4         9    12 e     pickup\n 4 dodge        ram 150~   4.7  2008     8 manua~ 4         9    12 e     pickup\n 5 honda        civic      1.8  2008     4 auto(~ f        25    36 r     subco~\n 6 honda        civic      1.8  2008     4 auto(~ f        24    36 c     subco~\n 7 jeep         grand c~   4.7  2008     8 auto(~ 4         9    12 e     suv   \n 8 toyota       corolla    1.8  2008     4 manua~ f        28    37 r     compa~\n 9 volkswagen   jetta      1.9  1999     4 manua~ f        33    44 d     compa~\n10 volkswagen   new bee~   1.9  1999     4 manua~ f        35    44 d     subco~\n11 volkswagen   new bee~   1.9  1999     4 auto(~ f        29    41 d     subco~\n\n\nAccording to the percentiles technique, there are 11 possible anomalies.\nTo decrease this count, you have the option to adjust the percentiles to 1 and 99.\n\nlower_bound &lt;- quantile(dat$hwy, 0.01)\nupper_bound &lt;- quantile(dat$hwy, 0.99)\n\noutlier_ind &lt;- which(dat$hwy &lt; lower_bound | dat$hwy &gt; upper_bound)\n\ndat[outlier_ind, ]\n\n# A tibble: 3 x 11\n  manufacturer model      displ  year   cyl trans  drv     cty   hwy fl    class\n  &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1 volkswagen   jetta        1.9  1999     4 manua~ f        33    44 d     comp~\n2 volkswagen   new beetle   1.9  1999     4 manua~ f        35    44 d     subc~\n3 volkswagen   new beetle   1.9  1999     4 auto(~ f        29    41 d     subc~\n\n\n\n\n\nThe Grubbs test identifies whether the extreme values in a dataset, either the highest or lowest, are outliers.\nThe Grubbs test examines outliers individually, focusing on one extreme value (either the highest or lowest) at a time. Therefore, the null hypothesis suggests that the highest value is not considered an outlier, while the alternative hypothesis proposes that the highest value is indeed an outlier.\nFor any statistical test, when the p-value falls below the selected significance threshold (usually α = 0.05), we reject the null hypothesis and infer that the lowest/highest value represents an outlier.\nConversely, if the p-value is equal to or greater than the significance level, we do not reject the null hypothesis. In this case, we conclude that, based on the data, there’s insufficient evidence to reject the idea that the lowest/highest value is not an outlier.\n\nlibrary(outliers)\n\nWarning: package 'outliers' was built under R version 4.1.3\n\ntest &lt;- grubbs.test(dat$hwy)\ntest\n\n\n    Grubbs test for one outlier\n\ndata:  dat$hwy\nG = 3.45274, U = 0.94862, p-value = 0.05555\nalternative hypothesis: highest value 44 is an outlier\n\n\nThe p-value of 0.056 suggests that, at a 5% significance level, we fail to reject the hypothesis indicating that the highest value of 44 might not be considered an outlier.\n\n\n\nLike the Grubb’s test, the Dixon test is utilized to determine if a particular low or high value is an outlier. If there are suspicions of multiple outliers, this test needs to be conducted separately on each suspected outlier.\nThe Dixon test is particularly beneficial when dealing with limited sample sizes, typically when n is equal to or less than 25.\n\nsubdat &lt;- dat[1:20, ]\ntest &lt;- dixon.test(subdat$hwy)\ntest\n\n\n    Dixon test for outliers\n\ndata:  subdat$hwy\nQ = 0.57143, p-value = 0.006508\nalternative hypothesis: lowest value 15 is an outlier\n\n\nThe findings indicate that the smallest figure, 15, stands out as an outlier (with a p-value of 0.007).\n\n\n\nMachine learning algorithms offer a diverse toolkit for anomaly detection due to their ability to comprehend complex patterns within datasets. Several specific algorithms, such as clustering, isolation forests, one-class SVM (Support Vector Machine), and neural networks, showcase proficiency in identifying anomalies by learning from the inherent structures and characteristics of the data.\n\n\nClustering algorithms, such as K-means or DBSCAN, group similar data points together based on certain similarity metrics. Anomalies are often isolated as data points that don’t fit into any distinct cluster or form clusters of their own, highlighting their dissimilarity from the majority of the data.\n\n\n\nOne-class SVM is specifically designed for anomaly detection by learning the patterns of normal data instances. It constructs a boundary around the normal data points in a high-dimensional space, effectively identifying anomalies as instances lying outside this boundary.\n\n\n\nNeural networks, particularly deep learning models, possess the capability to detect anomalies by learning intricate data representations. Autoencoders, a type of neural network architecture, are commonly used for anomaly detection. They reconstruct input data and pinpoint anomalies by their inability to accurately reconstruct, highlighting deviations from learned patterns.\n\n\n\n\n\nAnomaly and outlier detection play a pivotal role in data analysis across diverse domains. By leveraging statistical, machine learning, and visualization techniques, analysts can uncover hidden insights, potential errors, or significant events within datasets.\nUnderstanding the context and domain-specific knowledge are essential in defining anomalies and selecting suitable detection methods. Robust anomaly detection not only aids in identifying issues but also reveals valuable insights that might lead to improved decision-making, anomaly prevention, and enhanced overall data quality. Mastering anomaly detection techniques empowers analysts to extract meaningful information and glean deeper insights from their data, fostering more informed actions across various industries and disciplines."
  },
  {
    "objectID": "posts/Anomaly detection/index.html#methods-for-anomaly-detection",
    "href": "posts/Anomaly detection/index.html#methods-for-anomaly-detection",
    "title": "Anomaly Detection",
    "section": "",
    "text": "The dataset mpg from the ggplot2 package will be used to illustrate the different approaches of outliers detection in R, and in particular we will focus on the variable hwy (highway miles per gallon).\n\n\nThe initial stage in identifying outliers within R involves commencing with descriptive statistics, specifically focusing on the lowest and highest values, also known as the minimum and maximum.\nIn R, this can easily be done with the summary() function:\n\nlibrary(ggplot2)\ndat &lt;- ggplot2::mpg\nsummary(dat$hwy)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   18.00   24.00   23.44   27.00   44.00 \n\n\nThis simple technique can readily identify obvious encoding errors, such as a human’s weight listed as 786 kg (equivalent to 1733 pounds).\n\n\n\nAn alternative fundamental method for identifying outliers involves creating a histogram representing the data.\n\nhist(dat$hwy,\n  xlab = \"hwy\",\n  main = \"Histogram of hwy\",\n  breaks = sqrt(nrow(dat))\n)\n\n\n\n\nAccording to the histogram, there are a few observations that stand out as they are taller than the rest (notably the bar located on the right-hand side of the chart).\n\n\n\nBoxplots are great for spotting anomalies. They show data distribution and outliers well, helping identify observations that fall far outside the expected range. Anomalies often lie far beyond the “whiskers” of the boxplot, making it easy to visually detect them in a dataset.\n\nggplot(dat) +\n  aes(x = \"\", y = hwy) +\n  geom_boxplot(fill = \"blue\") +\n  theme_minimal()\n\n\n\n\nA boxplot illustrates a quantitative variable by presenting five typical summary statistics (minimum, median, first and third quartiles, and maximum), along with any data points identified as possible outliers using the interquartile range (IQR) rule.\nData points identified as potential outliers based on the IQR rule are represented as individual points in the boxplot. According to this criterion, there are two potential outliers, which are visible as the two points positioned above the upper end of the boxplot.\nIt is also possible to extract the values of the potential outliers based on the IQR criterion thanks to the boxplot.stats()$out function:\n\nboxplot.stats(dat$hwy)$out\n\n[1] 44 44 41\n\n\nWith the use of the which() function, it becomes feasible to retrieve the row number that corresponds to these exceptional values. Now armed with this information, you can readily revisit particular rows within the dataset to confirm them or display all variables related to these outliers.\n\nout &lt;- boxplot.stats(dat$hwy)$out\nout_ind &lt;- which(dat$hwy %in% c(out))\ndat[out_ind, ]\n\n# A tibble: 3 x 11\n  manufacturer model      displ  year   cyl trans  drv     cty   hwy fl    class\n  &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1 volkswagen   jetta        1.9  1999     4 manua~ f        33    44 d     comp~\n2 volkswagen   new beetle   1.9  1999     4 manua~ f        35    44 d     subc~\n3 volkswagen   new beetle   1.9  1999     4 auto(~ f        29    41 d     subc~\n\n\n\n\n\nThis outlier detection technique relies on using percentiles. Employing this method involves identifying potential outliers by considering all data points lying outside the range established by the 2.5th and 97.5th percentiles.\nAlternatively, different percentile ranges, like the 1st and 99th, or the 5th and 95th, can also be utilized to create the range for outlier detection.\nTo compute the values for the lower and upper percentiles (and consequently establish the boundaries of the range), the quantile() function can be employed.\n\nlower_bound &lt;- quantile(dat$hwy, 0.025)\nlower_bound\n\n2.5% \n  14 \n\nupper_bound &lt;- quantile(dat$hwy, 0.975)\nupper_bound\n\n 97.5% \n35.175 \n\n\nBased on this approach, any observations lower than 14 or higher than 35.175 will be regarded as potential outliers.\nAll variables considered as outliers with this method are as below:\n\noutlier_ind &lt;- which(dat$hwy &lt; lower_bound | dat$hwy &gt; upper_bound)\ndat[outlier_ind, ]\n\n# A tibble: 11 x 11\n   manufacturer model    displ  year   cyl trans  drv     cty   hwy fl    class \n   &lt;chr&gt;        &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n 1 dodge        dakota ~   4.7  2008     8 auto(~ 4         9    12 e     pickup\n 2 dodge        durango~   4.7  2008     8 auto(~ 4         9    12 e     suv   \n 3 dodge        ram 150~   4.7  2008     8 auto(~ 4         9    12 e     pickup\n 4 dodge        ram 150~   4.7  2008     8 manua~ 4         9    12 e     pickup\n 5 honda        civic      1.8  2008     4 auto(~ f        25    36 r     subco~\n 6 honda        civic      1.8  2008     4 auto(~ f        24    36 c     subco~\n 7 jeep         grand c~   4.7  2008     8 auto(~ 4         9    12 e     suv   \n 8 toyota       corolla    1.8  2008     4 manua~ f        28    37 r     compa~\n 9 volkswagen   jetta      1.9  1999     4 manua~ f        33    44 d     compa~\n10 volkswagen   new bee~   1.9  1999     4 manua~ f        35    44 d     subco~\n11 volkswagen   new bee~   1.9  1999     4 auto(~ f        29    41 d     subco~\n\n\nAccording to the percentiles technique, there are 11 possible anomalies.\nTo decrease this count, you have the option to adjust the percentiles to 1 and 99.\n\nlower_bound &lt;- quantile(dat$hwy, 0.01)\nupper_bound &lt;- quantile(dat$hwy, 0.99)\n\noutlier_ind &lt;- which(dat$hwy &lt; lower_bound | dat$hwy &gt; upper_bound)\n\ndat[outlier_ind, ]\n\n# A tibble: 3 x 11\n  manufacturer model      displ  year   cyl trans  drv     cty   hwy fl    class\n  &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1 volkswagen   jetta        1.9  1999     4 manua~ f        33    44 d     comp~\n2 volkswagen   new beetle   1.9  1999     4 manua~ f        35    44 d     subc~\n3 volkswagen   new beetle   1.9  1999     4 auto(~ f        29    41 d     subc~\n\n\n\n\n\nThe Grubbs test identifies whether the extreme values in a dataset, either the highest or lowest, are outliers.\nThe Grubbs test examines outliers individually, focusing on one extreme value (either the highest or lowest) at a time. Therefore, the null hypothesis suggests that the highest value is not considered an outlier, while the alternative hypothesis proposes that the highest value is indeed an outlier.\nFor any statistical test, when the p-value falls below the selected significance threshold (usually α = 0.05), we reject the null hypothesis and infer that the lowest/highest value represents an outlier.\nConversely, if the p-value is equal to or greater than the significance level, we do not reject the null hypothesis. In this case, we conclude that, based on the data, there’s insufficient evidence to reject the idea that the lowest/highest value is not an outlier.\n\nlibrary(outliers)\n\nWarning: package 'outliers' was built under R version 4.1.3\n\ntest &lt;- grubbs.test(dat$hwy)\ntest\n\n\n    Grubbs test for one outlier\n\ndata:  dat$hwy\nG = 3.45274, U = 0.94862, p-value = 0.05555\nalternative hypothesis: highest value 44 is an outlier\n\n\nThe p-value of 0.056 suggests that, at a 5% significance level, we fail to reject the hypothesis indicating that the highest value of 44 might not be considered an outlier.\n\n\n\nLike the Grubb’s test, the Dixon test is utilized to determine if a particular low or high value is an outlier. If there are suspicions of multiple outliers, this test needs to be conducted separately on each suspected outlier.\nThe Dixon test is particularly beneficial when dealing with limited sample sizes, typically when n is equal to or less than 25.\n\nsubdat &lt;- dat[1:20, ]\ntest &lt;- dixon.test(subdat$hwy)\ntest\n\n\n    Dixon test for outliers\n\ndata:  subdat$hwy\nQ = 0.57143, p-value = 0.006508\nalternative hypothesis: lowest value 15 is an outlier\n\n\nThe findings indicate that the smallest figure, 15, stands out as an outlier (with a p-value of 0.007).\n\n\n\nMachine learning algorithms offer a diverse toolkit for anomaly detection due to their ability to comprehend complex patterns within datasets. Several specific algorithms, such as clustering, isolation forests, one-class SVM (Support Vector Machine), and neural networks, showcase proficiency in identifying anomalies by learning from the inherent structures and characteristics of the data.\n\n\nClustering algorithms, such as K-means or DBSCAN, group similar data points together based on certain similarity metrics. Anomalies are often isolated as data points that don’t fit into any distinct cluster or form clusters of their own, highlighting their dissimilarity from the majority of the data.\n\n\n\nOne-class SVM is specifically designed for anomaly detection by learning the patterns of normal data instances. It constructs a boundary around the normal data points in a high-dimensional space, effectively identifying anomalies as instances lying outside this boundary.\n\n\n\nNeural networks, particularly deep learning models, possess the capability to detect anomalies by learning intricate data representations. Autoencoders, a type of neural network architecture, are commonly used for anomaly detection. They reconstruct input data and pinpoint anomalies by their inability to accurately reconstruct, highlighting deviations from learned patterns."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "In games involving chance, the concept of probability is easily understood. For instance, it’s intuitive to grasp that the odds of rolling a seven with a pair of dice are 1 in 6. However, beyond gaming, the meaning of probability varies in different contexts. Today, probability theory has a much broader application, becoming a common term in everyday language. Google’s auto-complete suggestions for “What are the chances of” include queries about having twins, rain, lightning strikes, and cancer.\nUnderstanding how to calculate probabilities provides an advantage in games of chance, leading many brilliant minds throughout history, including renowned mathematicians like Cardano, Fermat, and Pascal, to dedicate time and effort to unraveling the mathematics behind these games. This exploration gave birth to Probability Theory. Presently, probability remains incredibly relevant in contemporary games of chance, such as poker, where computing the probability of winning a hand based on visible cards is crucial. Casinos also heavily rely on probability theory to create games that almost certainly secure profits.\nHowever, probability theory extends far beyond gaming and is highly valuable in various contexts, especially those reliant on data influenced by chance in some manner.\n\n\nWhen we have information about the occurrence rates of various categories, establishing a distribution for categorical outcomes becomes fairly simple. We merely allocate a probability to each category. For situations resembling objects in a container (like beads in an urn), the distribution is determined by the proportion of each type of object.\nFor instance, if we’re randomly contacting potential voters from a population consisting of 44% Democrats, 44% Republicans, 10% undecided, and 2% Green Party members, these percentages serve as the probabilities for each group. Hence, the probability distribution is as follows:\n\n\n\nPr(picking a Democrat)\n0.44\n\n\nPr(picking a Republican)\n0.44\n\n\nPr(picking an undecided)\n0.10\n\n\nPr(picking a Green)\n0.02\n\n\n\n\n\n\nComputers offer the capability to carry out the described simple random experiment: for instance, selecting a bead randomly from a bag holding three blue beads and two red ones. Random number generators allow us to simulate this random selection process.\nAn illustration of this is the sample function in R. To showcase its application, we display its use in the code provided below. Initially, we utilize the function “rep” to create the collection of beads, resembling an urn, and subsequently employ “sample” to randomly select a bead from this simulated urn.\n\nbeads &lt;- rep(c(\"red\", \"blue\"), times = c(2,3))\nbeads\n\n[1] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\"\n\nsample(beads, 1)\n\n[1] \"red\"\n\n\nThis code generates a single random outcome. To simulate an infinite repetition of this experiment, which is practically impossible, we opt to repeat it a sufficiently large number of times, approximating an infinite scenario. This approach is an illustration of a Monte Carlo simulation.\nWe utilize the replicate function, enabling us to iterate the same action any desired number of times. In this instance, we repeat the random event X = 10,000 times. Subsequently, we verify if our defined definition aligns with the approximation derived from this Monte Carlo simulation. Here, prop.table gives us the proportions:\n\nX &lt;- 10000\nevents &lt;- replicate(X, sample(beads, 1))\ntab &lt;- table(events)\nprop.table(tab)\n\nevents\n  blue    red \n0.5961 0.4039 \n\n\nThe figures presented represent the estimated probabilities from this Monte Carlo simulation. According to statistical theory, as the value of ‘X’ increases, the approximations tend to converge towards 3/5, equivalent to 0.6, and 2/5, equivalent to 0.4.\n\n\n\nIn probability, events A and B are independent if the occurrence of one doesn’t influence the probability of the other. Mathematically, this means that the probability of both events happening is the product of their individual probabilities. This concept simplifies calculations but doesn’t imply the events are unrelated in reality.\nWhen you toss a fair coin and roll a fair six-sided die, these events are independent. The outcome of the coin toss (heads or tails) does not affect the outcome of rolling the die (getting a specific number). The probabilities of each event are not influenced by the other.\nDrawing cards from a deck without replacing them creates dependent events. For instance, after drawing a card from a standard deck without replacing it, the probability of drawing a specific card on the next draw changes because the deck’s composition has altered.\n\n\n\nWhen events are not independent, conditional probabilities are useful. We saw an example earlier where we talked about drawing two cards from a deck of cards without replacement. Consider:\nPr(Card 2 is a king ∣ Card 1 is a king ) = 3/51\nWe use the | as shorthand for “given that”.\nWhen two events, say are independent, we have:\nPr(A | B) = Pr(A)\nIn mathematical terms, this means that the occurrence of event B has no bearing on the probability of event A happening.\n\n\n\n\n\nThe addition rule tells us that:\nPr(A or B) = Pr(A) + Pr(B) - Pr(A and B)\nThis rule is easy to understand if you visualize a Venn diagram. When we add probabilities directly, we’re counting the overlapping part twice, thus necessitating the subtraction of one instance to correct for the duplication.\n\n\n\nIf we seek to determine the likelihood of two events, for instance, A and B, happening simultaneously, we can apply the multiplication rule:\nPr(A and B) = Pr(A) * Pr(B|A)\nSuppose you have a bag with 4 red balls and 6 blue balls. You draw one ball, note its color, and without replacing it, draw another ball. Let’s calculate the probability of drawing a red ball followed by drawing another red ball:\nFirst Event: Probability of drawing a red ball = Number of red balls / Total number of balls = 4 / 10 = 2/5.\nSecond Event: If the first ball drawn was red (which has happened with a probability of 2/5), now there are 3 red balls left out of 9 total balls. Therefore, the probability of drawing a second red ball after the first red ball was drawn = 3 / 9 = 1/3.\nNow, according to the multiplication rule, the overall probability of both events happening (drawing two red balls consecutively) is found by multiplying the probabilities of the individual events:\nProbability of drawing two red balls = Probability of the first red ball * Probability of the second red ball given the first was red = (2/5) * (1/3) = 2/15.\nSo, the probability of drawing two red balls consecutively, considering the dependency, is 2/15.\n\n\n\n\nNow let’s discuss one very prominent problems in probability theory: Birthday problem\nLet’s consider a scenario in a classroom containing 50 individuals. If we assume these 50 people were selected randomly, what is the probability that at least two individuals share the same birthday? To determine this probability, we’ll employ a Monte Carlo simulation. For the sake of simplicity, we disregard individuals born on February 29, as this adjustment doesn’t significantly alter the outcome.\nInitially, consider that birthdays can be depicted as values ranging from 1 to 365. Therefore, a collection of 50 birthdays can be acquired in the following manner. To verify whether there are at least two individuals with the same birthday within this specific group of 50 people, we can utilize the duplicated function. This function identifies duplicates within a vector and returns TRUE if any element is a duplicate.\n\nn &lt;- 50\nbdays &lt;- sample(1:365, n, replace = TRUE)\nduplicated(c(1,2,3,1,4,3,5))\n\n[1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\n\n\nTo check if two birthdays match, we can easily employ the any and duplicated functions in this manner:\n\nany(duplicated(bdays))\n\n[1] TRUE\n\n\nTo estimate the probability of having a common birthday within a group, we conduct multiple trials by selecting groups of 50 birthdays repeatedly.\n\nB &lt;- 10000\nsame_birthday &lt;- function(n){\n  bdays &lt;- sample(1:365, n, replace=TRUE)\n  any(duplicated(bdays))\n}\nresults &lt;- replicate(B, same_birthday(50))\nmean(results)\n\n[1] 0.9697\n\n\nWe can quickly create a function to compute this for any group size. By utilizing the sapply function, we can execute operations on individual elements using any specified function. Now, we’re able to create a graph illustrating the estimated probabilities of two individuals sharing a birthday within a gathering of size n.\n\ncompute_prob &lt;- function(n, B=10000){\n  results &lt;- replicate(B, same_birthday(n))\n  mean(results)\n}\n\nn &lt;- seq(1,60)\nprob &lt;- sapply(n, compute_prob)\n\nprob &lt;- sapply(n, compute_prob)\nqplot(n, prob)\n\n\n\n\nLet’s calculate precise probabilities instead of relying on Monte Carlo estimations. By employing mathematical methods, we obtain accurate solutions, and the calculations are significantly quicker because we’re not required to conduct experiments to generate results.\nFirst, considering the initial person, the likelihood of them having a distinct birthday is 1. Following this, the probability that the second person possesses a unique birthday, given that the first person has already selected one, is 364 out of 365. Subsequently, provided the first two individuals have distinct birthdays, the third person is left with 363 days to choose from. Continuing this pattern, the probability of all 50 people having different birthdays is then determined. Then the multiplication rule can be applied to find the probability of having a clash of birthdays.\nThe code can be written for n number of people:\n\nexact_prob &lt;- function(n){\n  prob_unique &lt;- seq(365,365-n+1)/365 \n  1 - prod( prob_unique)\n}\nexact_probs &lt;- sapply(n, exact_prob)\nqplot(n, prob) + geom_line(aes(n, exact_probs), col = \"green\")\n\n\n\n\nThis graph demonstrates that the Monte Carlo simulation offered a highly accurate estimation of the precise probability."
  },
  {
    "objectID": "posts/post-with-code/index.html#probability-distributions",
    "href": "posts/post-with-code/index.html#probability-distributions",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "When we have information about the occurrence rates of various categories, establishing a distribution for categorical outcomes becomes fairly simple. We merely allocate a probability to each category. For situations resembling objects in a container (like beads in an urn), the distribution is determined by the proportion of each type of object.\nFor instance, if we’re randomly contacting potential voters from a population consisting of 44% Democrats, 44% Republicans, 10% undecided, and 2% Green Party members, these percentages serve as the probabilities for each group. Hence, the probability distribution is as follows:\n\n\n\nPr(picking a Democrat)\n0.44\n\n\nPr(picking a Republican)\n0.44\n\n\nPr(picking an undecided)\n0.10\n\n\nPr(picking a Green)\n0.02"
  },
  {
    "objectID": "posts/post-with-code/index.html#monte-carlo-simulations-for-categorical-data",
    "href": "posts/post-with-code/index.html#monte-carlo-simulations-for-categorical-data",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Computers offer the capability to carry out the described simple random experiment: for instance, selecting a bead randomly from a bag holding three blue beads and two red ones. Random number generators allow us to simulate this random selection process.\nAn illustration of this is the sample function in R. To showcase its application, we display its use in the code provided below. Initially, we utilize the function “rep” to create the collection of beads, resembling an urn, and subsequently employ “sample” to randomly select a bead from this simulated urn.\n\nbeads &lt;- rep(c(\"red\", \"blue\"), times = c(2,3))\nbeads\n\n[1] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\"\n\nsample(beads, 1)\n\n[1] \"red\"\n\n\nThis code generates a single random outcome. To simulate an infinite repetition of this experiment, which is practically impossible, we opt to repeat it a sufficiently large number of times, approximating an infinite scenario. This approach is an illustration of a Monte Carlo simulation.\nWe utilize the replicate function, enabling us to iterate the same action any desired number of times. In this instance, we repeat the random event X = 10,000 times. Subsequently, we verify if our defined definition aligns with the approximation derived from this Monte Carlo simulation. Here, prop.table gives us the proportions:\n\nX &lt;- 10000\nevents &lt;- replicate(X, sample(beads, 1))\ntab &lt;- table(events)\nprop.table(tab)\n\nevents\n  blue    red \n0.5961 0.4039 \n\n\nThe figures presented represent the estimated probabilities from this Monte Carlo simulation. According to statistical theory, as the value of ‘X’ increases, the approximations tend to converge towards 3/5, equivalent to 0.6, and 2/5, equivalent to 0.4."
  },
  {
    "objectID": "posts/post-with-code/index.html#independence",
    "href": "posts/post-with-code/index.html#independence",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "In probability, events A and B are independent if the occurrence of one doesn’t influence the probability of the other. Mathematically, this means that the probability of both events happening is the product of their individual probabilities. This concept simplifies calculations but doesn’t imply the events are unrelated in reality.\nWhen you toss a fair coin and roll a fair six-sided die, these events are independent. The outcome of the coin toss (heads or tails) does not affect the outcome of rolling the die (getting a specific number). The probabilities of each event are not influenced by the other.\nDrawing cards from a deck without replacing them creates dependent events. For instance, after drawing a card from a standard deck without replacing it, the probability of drawing a specific card on the next draw changes because the deck’s composition has altered."
  },
  {
    "objectID": "posts/post-with-code/index.html#conditional-probabilities",
    "href": "posts/post-with-code/index.html#conditional-probabilities",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "When events are not independent, conditional probabilities are useful. We saw an example earlier where we talked about drawing two cards from a deck of cards without replacement. Consider:\nPr(Card 2 is a king ∣ Card 1 is a king ) = 3/51\nWe use the | as shorthand for “given that”.\nWhen two events, say are independent, we have:\nPr(A | B) = Pr(A)\nIn mathematical terms, this means that the occurrence of event B has no bearing on the probability of event A happening."
  },
  {
    "objectID": "posts/post-with-code/index.html#arithmetic-rules",
    "href": "posts/post-with-code/index.html#arithmetic-rules",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "The addition rule tells us that:\nPr(A or B) = Pr(A) + Pr(B) - Pr(A and B)\nThis rule is easy to understand if you visualize a Venn diagram. When we add probabilities directly, we’re counting the overlapping part twice, thus necessitating the subtraction of one instance to correct for the duplication.\n\n\n\nIf we seek to determine the likelihood of two events, for instance, A and B, happening simultaneously, we can apply the multiplication rule:\nPr(A and B) = Pr(A) * Pr(B|A)\nSuppose you have a bag with 4 red balls and 6 blue balls. You draw one ball, note its color, and without replacing it, draw another ball. Let’s calculate the probability of drawing a red ball followed by drawing another red ball:\nFirst Event: Probability of drawing a red ball = Number of red balls / Total number of balls = 4 / 10 = 2/5.\nSecond Event: If the first ball drawn was red (which has happened with a probability of 2/5), now there are 3 red balls left out of 9 total balls. Therefore, the probability of drawing a second red ball after the first red ball was drawn = 3 / 9 = 1/3.\nNow, according to the multiplication rule, the overall probability of both events happening (drawing two red balls consecutively) is found by multiplying the probabilities of the individual events:\nProbability of drawing two red balls = Probability of the first red ball * Probability of the second red ball given the first was red = (2/5) * (1/3) = 2/15.\nSo, the probability of drawing two red balls consecutively, considering the dependency, is 2/15."
  },
  {
    "objectID": "posts/post-with-code/index.html#example",
    "href": "posts/post-with-code/index.html#example",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Now let’s discuss one very prominent problems in probability theory: Birthday problem\nLet’s consider a scenario in a classroom containing 50 individuals. If we assume these 50 people were selected randomly, what is the probability that at least two individuals share the same birthday? To determine this probability, we’ll employ a Monte Carlo simulation. For the sake of simplicity, we disregard individuals born on February 29, as this adjustment doesn’t significantly alter the outcome.\nInitially, consider that birthdays can be depicted as values ranging from 1 to 365. Therefore, a collection of 50 birthdays can be acquired in the following manner. To verify whether there are at least two individuals with the same birthday within this specific group of 50 people, we can utilize the duplicated function. This function identifies duplicates within a vector and returns TRUE if any element is a duplicate.\n\nn &lt;- 50\nbdays &lt;- sample(1:365, n, replace = TRUE)\nduplicated(c(1,2,3,1,4,3,5))\n\n[1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\n\n\nTo check if two birthdays match, we can easily employ the any and duplicated functions in this manner:\n\nany(duplicated(bdays))\n\n[1] TRUE\n\n\nTo estimate the probability of having a common birthday within a group, we conduct multiple trials by selecting groups of 50 birthdays repeatedly.\n\nB &lt;- 10000\nsame_birthday &lt;- function(n){\n  bdays &lt;- sample(1:365, n, replace=TRUE)\n  any(duplicated(bdays))\n}\nresults &lt;- replicate(B, same_birthday(50))\nmean(results)\n\n[1] 0.9697\n\n\nWe can quickly create a function to compute this for any group size. By utilizing the sapply function, we can execute operations on individual elements using any specified function. Now, we’re able to create a graph illustrating the estimated probabilities of two individuals sharing a birthday within a gathering of size n.\n\ncompute_prob &lt;- function(n, B=10000){\n  results &lt;- replicate(B, same_birthday(n))\n  mean(results)\n}\n\nn &lt;- seq(1,60)\nprob &lt;- sapply(n, compute_prob)\n\nprob &lt;- sapply(n, compute_prob)\nqplot(n, prob)\n\n\n\n\nLet’s calculate precise probabilities instead of relying on Monte Carlo estimations. By employing mathematical methods, we obtain accurate solutions, and the calculations are significantly quicker because we’re not required to conduct experiments to generate results.\nFirst, considering the initial person, the likelihood of them having a distinct birthday is 1. Following this, the probability that the second person possesses a unique birthday, given that the first person has already selected one, is 364 out of 365. Subsequently, provided the first two individuals have distinct birthdays, the third person is left with 363 days to choose from. Continuing this pattern, the probability of all 50 people having different birthdays is then determined. Then the multiplication rule can be applied to find the probability of having a clash of birthdays.\nThe code can be written for n number of people:\n\nexact_prob &lt;- function(n){\n  prob_unique &lt;- seq(365,365-n+1)/365 \n  1 - prod( prob_unique)\n}\nexact_probs &lt;- sapply(n, exact_prob)\nqplot(n, prob) + geom_line(aes(n, exact_probs), col = \"green\")\n\n\n\n\nThis graph demonstrates that the Monte Carlo simulation offered a highly accurate estimation of the precise probability."
  },
  {
    "objectID": "posts/post-with-code/index.html#probability-distribution-of-a-random-variable",
    "href": "posts/post-with-code/index.html#probability-distribution-of-a-random-variable",
    "title": "Probability Theory and Random Variables",
    "section": "Probability distribution of a random variable",
    "text": "Probability distribution of a random variable\nThe probability distribution of a random variable describes the likelihood of different outcomes or values that the variable can take on, along with their associated probabilities. It provides a comprehensive overview of the probabilities associated with each possible value of the random variable. More specifically, the probability distribution of a random variable, say S, tells us the probability of the observed value falling at any given interval.\nIf we can define a cumulative distribution function \\(F(a) = Pr(S \\le a)\\), then we will be able to answer any question related to the probability of events defined by our random variable S. We call this F the random variable’s distribution function.\nWe can approximate the distribution function of the random variable by employing a Monte Carlo simulation, generating numerous instances of the random variable."
  },
  {
    "objectID": "posts/post-with-code/index.html#expected-value-and-standard-error",
    "href": "posts/post-with-code/index.html#expected-value-and-standard-error",
    "title": "Probability Theory and Random Variables",
    "section": "Expected value and standard error",
    "text": "Expected value and standard error\nIt is common to use letter E like this: \\[\nE[X]\n\\] to denote the expected value of the random variable X.\nA random variable fluctuates around its expected value, and with numerous iterations, the average of these iterations converges toward the expected value, becoming more precise with increased iterations.\nTheoretical statistics offers methods to compute expected values in various scenarios. For instance, a valuable formula indicates that the expected value of a random variable based on a single draw is the average of the numbers within the container or set.\nIn general, if an urn has two possible outcomes, say a and b, with proportions p and (1-p) respectively, the average is: \\[\nE[X] = ap + b(1-p)\n\\]\nThe standard error (SE) provides insight into the extent of variability around the expected value. In statistical literature, it’s typical to represent the standard error of a random variable as \\[\nSE[X]\n\\]\nIn general, if an urn has two possible outcomes, say a and b, with proportions p and (1-p) respectively, the average is: \\[\nSE[X] = |b-a|\\sqrt{p(1-p)}\n\\] ## Central Limit Theorem\nThe Central Limit Theorem (CLT) states that when the number of draws, known as the sample size, is sufficiently large, the probability distribution of the sum of independent draws tends to be close to a normal distribution. As sampling models are utilized across numerous data generation procedures, the CLT is widely regarded as one of the most significant and influential mathematical concepts in history.\nCLT operates effectively when the number of samples is considerable, but what qualifies as “large” is subjective. In various cases, as few as 30 samples might suffice for the CLT to be applicable, and in specific situations, even 10 samples could be adequate. It’s important to note that these are not universal guidelines. It’s worth considering that when the likelihood of success is extremely low, larger sample sizes become necessary."
  },
  {
    "objectID": "posts/post-with-code/index.html#law-of-large-numbers",
    "href": "posts/post-with-code/index.html#law-of-large-numbers",
    "title": "Probability Theory and Random Variables",
    "section": "Law of large numbers",
    "text": "Law of large numbers\nAs the sample size (n) increases, the standard error of the mean decreases. When n becomes sufficiently large, the standard error approaches zero, and the average of the samples tends to match the average of the entire population. This phenomenon is referred to in statistical literature as the law of large numbers or the law of averages.\nThe misconception surrounding the law of averages often leads to misinterpretations. For instance, if you flip a coin five times and observe heads each time, someone might mistakenly argue that the next toss is likely to result in tails because, on average, there should be a 50-50 split between heads and tails. Similarly, there’s a tendency to anticipate a red outcome on a roulette wheel after witnessing five consecutive black spins. However, these events are independent, so the chance of a coin landing heads remains 50%, regardless of the previous outcomes. This principle also applies to roulette. The law of averages is relevant only when the number of trials is significantly large, not in small samples. For instance, after a million coin tosses, you’re expected to see approximately 50% heads, irrespective of the initial five tosses.\nAnother funny misapplication of the law of averages occurs in sports when television sportscasters predict a player’s imminent success just because they’ve encountered a few failures in a row."
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "In the vast landscape of machine learning, classification stands tall as a fundamental and powerful technique. It’s a predictive modeling method that allows algorithms to learn patterns from labeled data and categorize new, unseen data into predefined classes or categories. From spam email detection to medical diagnosis and sentiment analysis, classification algorithms play a pivotal role in numerous real-world applications.\nClassification is a supervised learning technique wherein the algorithm learns from labeled training data to predict the class or category of new, unseen instances. It involves assigning labels or categories to data points based on their features or attributes.\n\n\nNumerous algorithms cater to classification tasks, each with its strengths and suitability for different scenarios. Some of the widely used ones include:\n\n\nLogistic regression estimates the probability (ranging from 0 to 1) of an instance belonging to a specific class, unlike linear regression that predicts continuous values. Achieving this probability estimation involves using the logistic function (sigmoid function).\nTo classify instances, logistic regression applies a threshold to these probability scores. For instance, if the probability exceeds 0.5, it assigns the instance to one class; otherwise, it assigns it to the other, establishing a decision boundary.\nIn modeling the relationship between features and the logarithm of event odds, logistic regression assigns coefficients to each feature. These coefficients signify the impact of features on the outcome, aiding in understanding feature importance during classification. We’ll utilize a dataset containing details about patients susceptible to or not susceptible to strokes. Specifically, we’re employing the Stroke Prediction Dataset obtained from Kaggle to conduct predictions. Our objective involves analyzing the patient data within the training set to forecast whether a patient within the evaluation set is prone to experiencing a stroke or not.\n\ndf_stroke&lt;-read.csv(\"healthcare-dataset-stroke-data.csv\")\nglimpse(df_stroke)\n\nRows: 5,110\nColumns: 12\n$ id                &lt;int&gt; 9046, 51676, 31112, 60182, 1665, 56669, 53882, 10434…\n$ gender            &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Female\", \"Female\", \"Male\"…\n$ age               &lt;dbl&gt; 67, 61, 80, 49, 79, 81, 74, 69, 59, 78, 81, 61, 54, …\n$ hypertension      &lt;int&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1…\n$ heart_disease     &lt;int&gt; 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0…\n$ ever_married      &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No…\n$ work_type         &lt;chr&gt; \"Private\", \"Self-employed\", \"Private\", \"Private\", \"S…\n$ Residence_type    &lt;chr&gt; \"Urban\", \"Rural\", \"Rural\", \"Urban\", \"Rural\", \"Urban\"…\n$ avg_glucose_level &lt;dbl&gt; 228.69, 202.21, 105.92, 171.23, 174.12, 186.21, 70.0…\n$ bmi               &lt;chr&gt; \"36.6\", \"N/A\", \"32.5\", \"34.4\", \"24\", \"29\", \"27.4\", \"…\n$ smoking_status    &lt;chr&gt; \"formerly smoked\", \"never smoked\", \"never smoked\", \"…\n$ stroke            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nOur data glimpse shows that we have 5110 observations and 12 variables.\nWe can generate several bar graphs to visualize the connections between each of these factors and the target variable, which is the likelihood of a stroke occurrence in the individual.\n\np1 &lt;- ggplot(data = df_stroke) +geom_bar(mapping = aes(x = gender,fill=stroke))\np2 &lt;-ggplot(data = df_stroke) +geom_bar(mapping = aes(x = hypertension,fill=stroke))\np3 &lt;-ggplot(data = df_stroke) +geom_bar(mapping = aes(x = heart_disease,fill=stroke)) \np4 &lt;-ggplot(data = df_stroke) +geom_bar(mapping = aes(x = ever_married,fill=stroke)) \ngrid.arrange(p1,p2,p3,p4 ,ncol= 2)\n\n\n\np5 &lt;- ggplot(data = df_stroke) +geom_bar(mapping = aes(x = work_type,fill=stroke))\np6 &lt;-ggplot(data = df_stroke) +geom_bar(mapping = aes(x = Residence_type,fill=stroke))\np7 &lt;-ggplot(data = df_stroke) +geom_bar(mapping = aes(x = smoking_status,fill=stroke)) \ngrid.arrange(p5,p6,p7 ,ncol= 1)\n\n\n\n\nNow we prepare the training and test sets from the data.\n\ndf_stroke &lt;- na.omit(df_stroke)\nn_obs &lt;- nrow(df_stroke)\nsplit &lt;- round(n_obs * 0.7)\ntrain &lt;- df_stroke[1:split,]\n# Create test\ntest &lt;- df_stroke[(split + 1):nrow(df_stroke),]\n\n\nmodel &lt;- logistic_reg(mixture = double(1), penalty = double(1)) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"classification\") %&gt;%\n  fit(stroke ~ ., data = train)\n\n#confusionMatrix(factor(as.list(predict(model, test, type = \"class\"))), factor(test$stroke))\n\n\n\n\nDecision trees in classification start by selecting the best attribute at each node to divide the data, using measures like Gini impurity or information gain. This recursive process continues until the data is segregated into distinct subsets or meets certain stopping conditions, such as a specified tree depth or a minimum number of samples in a node.\nIn the classification process, new data instances navigate through the tree based on their attribute values. As they traverse the branches, they eventually arrive at a leaf node, where the decision tree assigns a predicted class label based on the attributes of the instance.\nOne of the advantages of decision trees is their ability to handle both categorical and numerical data. For categorical attributes, decision trees consider all possible outcomes for splitting, while for numerical attributes, the trees choose specific thresholds to partition the data efficiently.\n\n\n\nAn ensemble method comprising multiple decision trees, resulting in robust and accurate predictions.\nTo set up the model, we call the randomForest classifier and point it to ‘stroke’ column for the outcome and provide the ‘train’ set as input.\n\nrf_model&lt;-randomForest(formula= stroke~.,data = train)\n\nAfter training the model using the training set, it’s essential to assess its performance on comparable data. To achieve this, we’ll utilize the test dataset. Let’s display the confusion matrix to analyze how effectively our classification model worked when applied to the test data.\n\nconfusionMatrix(predict(rf_model, test), test$stroke)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   No  Yes\n       No  1469    0\n       Yes    3    0\n                                          \n               Accuracy : 0.998           \n                 95% CI : (0.9941, 0.9996)\n    No Information Rate : 1               \n    P-Value [Acc &gt; NIR] : 1.0000          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : 0.2482          \n                                          \n            Sensitivity : 0.998           \n            Specificity :    NA           \n         Pos Pred Value :    NA           \n         Neg Pred Value :    NA           \n             Prevalence : 1.000           \n         Detection Rate : 0.998           \n   Detection Prevalence : 0.998           \n      Balanced Accuracy :    NA           \n                                          \n       'Positive' Class : No              \n                                          \n\n\n\n\n\nSVM operates by identifying the hyperplane that maximizes the margin, which is the distance between the hyperplane and the nearest data points of each class, also known as support vectors. This hyperplane, while linear in nature for the basic implementation, aims to create the largest separation between classes, providing robustness in classifying unseen data.\nIn its standard form, SVM performs linear classification. However, by using kernel functions like polynomial, radial basis function (RBF), or sigmoid, SVMs can efficiently handle non-linear relationships by transforming the input data into higher-dimensional spaces where the classes become linearly separable.\n\nclassifier = svm(formula = stroke ~ ., \n                 data = train, \n                 type = 'C-classification', \n                 kernel = 'linear') \n\nconfusionMatrix(predict(classifier, test), test$stroke)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   No  Yes\n       No  1472    0\n       Yes    0    0\n                                     \n               Accuracy : 1          \n                 95% CI : (0.9975, 1)\n    No Information Rate : 1          \n    P-Value [Acc &gt; NIR] : 1          \n                                     \n                  Kappa : NaN        \n                                     \n Mcnemar's Test P-Value : NA         \n                                     \n            Sensitivity :  1         \n            Specificity : NA         \n         Pos Pred Value : NA         \n         Neg Pred Value : NA         \n             Prevalence :  1         \n         Detection Rate :  1         \n   Detection Prevalence :  1         \n      Balanced Accuracy : NA         \n                                     \n       'Positive' Class : No         \n                                     \n\n\n\n\n\nNeural networks process data using layers of neurons. The input layer receives data, hidden layers process information, and the output layer produces predictions.\nActivation functions add non-linearities to neurons, enabling the model to learn complex data patterns and relationships.\nThrough training, neural networks adjust internal parameters (weights and biases) by comparing predicted outputs with actual labels using optimization algorithms like gradient descent.\nDeep neural networks, with multiple hidden layers, excel in capturing intricate data features and hierarchies, making them well-suited for handling complex classification tasks.\n\n\n\n\nIn conclusion, machine learning classification offers diverse tools like SVM, Decision Trees, and Neural Networks, vital for informed decision-making across industries. Understanding these methods and their strengths is crucial for effective use. Challenges like overfitting and data imbalance require careful handling. Mastering these techniques empowers better insights, automation, and innovation in various domains, marking a fundamental aspect of data-driven problem-solving."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering involves organizing a set of items so that those within the same group (called a cluster) are more alike to each other than to those in different groups. It is frequently employed by data experts during exploratory data analysis to uncover fresh insights and trends within data. Since clustering falls under unsupervised machine learning, it operates without the need for labeled datasets.\nClustering, in contrast to supervised learning tasks like classification or regression, lacks a complete end-to-end automation capability. Instead, it involves an iterative process of uncovering information that necessitates expertise in the field and frequent human judgment for adjusting both data and model parameters to achieve the desired outcomes.\nCrucially, because clustering operates on unsupervised learning principles and doesn’t rely on labeled data, it’s impossible to compute performance metrics like accuracy, AUC, RMSE, etc., for comparing various algorithms or data preprocessing methods. Consequently, evaluating clustering models becomes notably challenging and subjective.\nThe primary benchmarks for successful clustering models revolve around:\n\nIs the model interpretable?\nDoes the clustering output contribute to business utility?\nHave novel insights or previously undiscovered patterns in the data been revealed through clustering?\n\nBefore diving into algorithmic details, let’s first develop a basic understanding of clustering by employing a simplified example involving a dataset of various fruits. Suppose we possess a large assortment of images featuring three types of fruits: strawberries, pears, and apples.\nWithin this dataset, the images are jumbled together, and the objective is to categorize similar fruits into three distinct groups, with each group exclusively containing one type of fruit. This is exactly what a clustering algorithm will do.\n\n\n\n\n\nThe initial phase of any clustering algorithm involves establishing a measure of distance between individual observations or sets of observations. Subsequently, determining the method to unite these observations into clusters becomes essential. Various algorithms exist for this purpose, with hierarchical and k-means being two such examples.\nTo illustrate this process, we will create a basic scenario using movie ratings. Specifically, we’ll generate a matrix, x, containing ratings for the top 50 movies based on the highest number of ratings received. The 50 movies are as follows:\n\ndata(\"movielens\")\ntop &lt;- movielens |&gt;\n  group_by(movieId) |&gt;\n  summarize(n=n(), title = first(title)) |&gt;\n  top_n(50, n) |&gt;\n  pull(movieId)\n\nx &lt;- movielens |&gt; \n  filter(movieId %in% top) |&gt;\n  group_by(userId) |&gt;\n  filter(n() &gt;= 25) |&gt;\n  ungroup() |&gt; \n  select(title, userId, rating) |&gt;\n  spread(userId, rating)\n\nrow_names &lt;- str_remove(x$title, \": Episode\") |&gt; str_trunc(20)\nx &lt;- x[,-1] |&gt; as.matrix()\nx &lt;- sweep(x, 2, colMeans(x, na.rm = TRUE))\nx &lt;- sweep(x, 1, rowMeans(x, na.rm = TRUE))\nrownames(x) &lt;- row_names\n\nrow_names\n\n [1] \"Ace Ventura: Pet ...\" \"Aladdin\"              \"American Beauty\"     \n [4] \"Apollo 13\"            \"Back to the Future\"   \"Batman\"              \n [7] \"Beauty and the Beast\" \"Braveheart\"           \"Dances with Wolves\"  \n[10] \"Dumb & Dumber (Du...\" \"E.T. the Extra-Te...\" \"Fargo\"               \n[13] \"Fight Club\"           \"Forrest Gump\"         \"Fugitive, The\"       \n[16] \"Gladiator\"            \"Godfather, The\"       \"Good Will Hunting\"   \n[19] \"Groundhog Day\"        \"Independence Day ...\" \"Jurassic Park\"       \n[22] \"Lion King, The\"       \"Lord of the Rings...\" \"Lord of the Rings...\"\n[25] \"Lord of the Rings...\" \"Mask, The\"            \"Matrix, The\"         \n[28] \"Men in Black (a.k...\" \"Mission: Impossible\"  \"Princess Bride, The\" \n[31] \"Pulp Fiction\"         \"Raiders of the Lo...\" \"Saving Private Ryan\" \n[34] \"Schindler's List\"     \"Seven (a.k.a. Se7en)\" \"Shawshank Redempt...\"\n[37] \"Shrek\"                \"Silence of the La...\" \"Sixth Sense, The\"    \n[40] \"Speed\"                \"Star Wars IV - A ...\" \"Star Wars V - The...\"\n[43] \"Star Wars VI - Re...\" \"Terminator 2: Jud...\" \"Terminator, The\"     \n[46] \"Titanic\"              \"Toy Story\"            \"True Lies\"           \n[49] \"Twelve Monkeys (a...\" \"Usual Suspects, The\" \n\n\nWe aim to utilize this data to identify potential groups or clusters of movies by analyzing the ratings given by 139 movie reviewers. Our initial approach involves determining the distance between every pair of movies using the dist function.\n\nd &lt;- dist(x)\n\n\n\nAfter calculating the distance between every pair of movies, we require a method to create clusters based on this information. Hierarchical clustering begins by treating each movie as its own cluster. Subsequently, it progressively merges the two closest clusters together until eventually forming a single cluster containing all movies. The hclust function executes this process and requires a distance measure as its input. A dendrogram allows us to observe the groups that have emerged as a result.\n\nh &lt;- hclust(d)\nplot(h, cex = 0.65, main = \"\", xlab = \"\")\n\n\n\n\nThis chart provides an estimation of the distance separating any two films. This distance is determined by identifying the first point, from top to bottom, where the movies diverge into distinct categories. The vertical position of this point indicates the gap between these groups. For instance, the distance between the three Star Wars movies is 8 units or fewer, while the gap between Raiders of the Lost Ark and Silence of the Lambs is approximately 17.\nTo establish specific groups, there are two approaches:\n\nspecifying a minimum required distance for observations to belong to the same group, or\ndetermining the desired number of groups and then identifying the minimum distance that accomplishes this.\n\nThe function cutree can be applied to the outcome of hclust to execute either of these methods and create distinct groups.\n\ngroups &lt;- cutree(h, k = 10)\n\nObserve that the clustering offers valuable understanding regarding different movie categories. Group 4 seems to consist of blockbuster films:\n\nnames(groups)[groups==4]\n\n[1] \"Apollo 13\"            \"Braveheart\"           \"Dances with Wolves\"  \n[4] \"Forrest Gump\"         \"Good Will Hunting\"    \"Saving Private Ryan\" \n[7] \"Schindler's List\"     \"Shawshank Redempt...\"\n\n\nGroup 9 seems to consist of films that cater to nerdy or geeky interests:\n\nnames(groups)[groups==9]\n\n[1] \"Lord of the Rings...\" \"Lord of the Rings...\" \"Lord of the Rings...\"\n[4] \"Star Wars IV - A ...\" \"Star Wars V - The...\" \"Star Wars VI - Re...\"\n\n\n\n\n\nIn order to employ the k-means clustering technique, it’s essential to pre-specify the number of clusters k, that we aim to establish. The k-means algorithm is iterative. Initially, k centers are designated. Subsequently, every data point is allocated to the cluster whose center is nearest to that particular data point. Then, in the following step, the centers are re-calibrated using the observations within each cluster, where the average values for each feature are utilized to establish a centroid. These two steps are reiterated until the the centers converge. The different can be visualized below\n\nk &lt;- kmeans(x_0, centers = 10)\ngroups &lt;- k$cluster\n\nplotcluster(x_0, groups)"
  },
  {
    "objectID": "posts/clustering/index.html#hierarchical-clustering",
    "href": "posts/clustering/index.html#hierarchical-clustering",
    "title": "Clustering",
    "section": "",
    "text": "After calculating the distance between every pair of movies, we require a method to create clusters based on this information. Hierarchical clustering begins by treating each movie as its own cluster. Subsequently, it progressively merges the two closest clusters together until eventually forming a single cluster containing all movies. The hclust function executes this process and requires a distance measure as its input. A dendrogram allows us to observe the groups that have emerged as a result.\n\nh &lt;- hclust(d)\nplot(h, cex = 0.65, main = \"\", xlab = \"\")\n\n\n\n\nThis chart provides an estimation of the distance separating any two films. This distance is determined by identifying the first point, from top to bottom, where the movies diverge into distinct categories. The vertical position of this point indicates the gap between these groups. For instance, the distance between the three Star Wars movies is 8 units or fewer, while the gap between Raiders of the Lost Ark and Silence of the Lambs is approximately 17.\nTo establish specific groups, there are two approaches:\n\nspecifying a minimum required distance for observations to belong to the same group, or\ndetermining the desired number of groups and then identifying the minimum distance that accomplishes this.\n\nThe function cutree can be applied to the outcome of hclust to execute either of these methods and create distinct groups.\n\ngroups &lt;- cutree(h, k = 10)\n\nObserve that the clustering offers valuable understanding regarding different movie categories. Group 4 seems to consist of blockbuster films:\n\nnames(groups)[groups==4]\n\n[1] \"Apollo 13\"            \"Braveheart\"           \"Dances with Wolves\"  \n[4] \"Forrest Gump\"         \"Good Will Hunting\"    \"Saving Private Ryan\" \n[7] \"Schindler's List\"     \"Shawshank Redempt...\"\n\n\nGroup 9 seems to consist of films that cater to nerdy or geeky interests:\n\nnames(groups)[groups==9]\n\n[1] \"Lord of the Rings...\" \"Lord of the Rings...\" \"Lord of the Rings...\"\n[4] \"Star Wars IV - A ...\" \"Star Wars V - The...\" \"Star Wars VI - Re...\""
  },
  {
    "objectID": "posts/clustering/index.html#k-means-clustering",
    "href": "posts/clustering/index.html#k-means-clustering",
    "title": "Clustering",
    "section": "",
    "text": "In order to employ the k-means clustering technique, it’s essential to pre-specify the number of clusters k, that we aim to establish. The k-means algorithm is iterative. Initially, k centers are designated. Subsequently, every data point is allocated to the cluster whose center is nearest to that particular data point. Then, in the following step, the centers are re-calibrated using the observations within each cluster, where the average values for each feature are utilized to establish a centroid. These two steps are reiterated until the the centers converge. The different can be visualized below\n\nk &lt;- kmeans(x_0, centers = 10)\ngroups &lt;- k$cluster\n\nplotcluster(x_0, groups)"
  },
  {
    "objectID": "posts/clustering/index.html#customer-segmentation",
    "href": "posts/clustering/index.html#customer-segmentation",
    "title": "Clustering",
    "section": "Customer Segmentation",
    "text": "Customer Segmentation\nConsumers are grouped based on clustering algorithms that analyze their buying patterns or preferences, aiming to create targeted marketing strategies. For instance, when faced with a vast customer base like 10 million individuals, rather than crafting an impractical number of 10 million distinct marketing campaigns, clustering could be employed to condense these customers into 25 clusters, allowing for the creation of 25 tailored marketing campaigns instead."
  },
  {
    "objectID": "posts/clustering/index.html#retail-clustering",
    "href": "posts/clustering/index.html#retail-clustering",
    "title": "Clustering",
    "section": "Retail Clustering",
    "text": "Retail Clustering\nNumerous opportunities exist for clustering within retail operations. One approach involves collecting store-specific data and clustering it to identify similarities among stores based on factors such as foot traffic, average sales, and product variety. For instance, by grouping stores based on these attributes, insights can be derived regarding which locations share similarities."
  },
  {
    "objectID": "posts/clustering/index.html#image-segmentation",
    "href": "posts/clustering/index.html#image-segmentation",
    "title": "Clustering",
    "section": "Image Segmentation",
    "text": "Image Segmentation\nImage segmentation involves categorizing an image into distinct groups, and extensive studies have focused on employing clustering techniques within this field. This clustering method proves beneficial when aiming to separate objects within an image for individual analysis and identification purposes."
  },
  {
    "objectID": "posts/regression/index.html",
    "href": "posts/regression/index.html",
    "title": "Regression",
    "section": "",
    "text": "Linear Regression\nLinear regression stands as a fundamental concept in statistics and machine learning, serving as a cornerstone for predictive modeling. Its simplicity and effectiveness make it a widely used technique in various fields, from finance to healthcare and beyond. In this blog post, we’ll delve into the basics of linear regression, its applications, and how it works.\nAt its core, linear regression is a statistical method used to understand the relationship between two variables: an independent variable (predictor) and a dependent variable (outcome). The goal is to establish a linear relationship between these variables, enabling predictions and understanding the impact of changes in the predictor on the outcome.\nSimple linear regression involves one independent variable, while multiple linear regression deals with several predictors. Both aim to fit a linear equation to the data that best explains the relationship between variables. The equation takes the form: \\(Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\epsilon\\), where \\(Y\\) is the dependent variable, \\(X\\)s represent the predictors, \\(\\beta\\)s are coefficients, and \\(\\epsilon\\) is the error term.\nTo understand regression thoroughly we will use a data-driven approach that examines the height data from families to try to understand heredity.\nIf we were to specify a linear model for this data, we would denote the \\(N\\) observed father heights with \\(x_1, x_2, ....x_n\\), then we model the \\(N\\) son heights we are trying to predict with: \\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i, i = 1, 2, ....N.\\] where \\(Y_i\\) is the random son’s height that we want to predict.\nOne reason why linear models are popular is their interpretability. Regarding the above data, we can explain the data as follows: due to inherited genes, a son’s predicted height increases by \\(\\beta_1\\) for each additional inch in the father’s height, denoted by \\(x\\). However, not all sons with fathers of the same height \\(x\\) have identical heights. This disparity is accounted for by the term \\(\\epsilon\\), which represents the remaining variability. This variability encompasses factors like the mother’s genetic influence, environmental aspects, and other biological random variations.\nThe way we formulated the model indicates that the intercept \\(\\beta_0\\) lacks interpretability because it represents the predicted height of a son with a father having zero height, which is not practically meaningful in this context.\nIn order for linear models to be effective, it’s necessary to estimate the unknown values of \\(\\beta\\). The conventional scientific approach involves determining the values that minimize the difference between the model and the actual data. This process is known as the least squares (LS) equation, a concept that will be frequently encountered in this chapter. For our data, the equation can be represented as:\n\\[RSS = \\sum_{i=1}^{n} (y_i - (β_0 + β_1x_1))^2\\]\nThis calculation represents the residual sum of squares (RSS). Once we identify the values that minimize the RSS, these values are referred to as the least squares estimates (LSE).\nWithin the R programming language, the least squares estimates can be acquired by utilizing the lm function.\n\ndata(\"GaltonFamilies\")\nset.seed(1983)\ngalton_heights &lt;- GaltonFamilies |&gt;\n  filter(gender == \"male\") |&gt;\n  group_by(family) |&gt;\n  sample_n(1) |&gt;\n  ungroup() |&gt;\n  select(father, childHeight) |&gt;\n  rename(son = childHeight)\nfit &lt;- lm(son ~ father, data = galton_heights)\nfit$coef\n\n(Intercept)      father \n  37.287605    0.461392 \n\n\nOn visualizing the heights of the fathers and their sons as well as the best linear fitting model, we can say that linear regressor has worked fine.\n\ngalton_heights |&gt; ggplot(aes(son, father)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x)\n\n\n\n\n\n\nNon - Linear Regression\nTraditional linear regression assumes a linear relationship between the independent and dependent variables, following a straight line or plane. However, real-world phenomena often exhibit more complex and curvilinear patterns. Non-linear regression, unlike its linear counterpart, allows for fitting models to non-linear data by employing curved lines, surfaces, or more intricate functions.\nUnderstanding when to opt for non-linear regression is pivotal. Many natural processes, such as biological, economic, or physical systems, display patterns that cannot be adequately represented by linear models. For instance, growth patterns, saturation effects, or exponential decay are common scenarios where non-linear regression becomes indispensable.\nNon-linear regression encompasses a wide spectrum of models. These include polynomial, exponential, logarithmic, power-law, sigmoidal, and many other complex functions. Each model is tailored to suit specific data behaviors, aiming to accurately capture and predict relationships that linear models cannot effectively represent.\nFor example, let’s create a custom data where \\[x = 1, 2, ....50\\] and \\[ y = x^3 - 2x^2 + x + 2 + \\epsilon\\] where \\(\\epsilon\\) denotes random noise.\n\nx &lt;- 1:50\ny &lt;- x^3 - 2 * x^2 + x + 2 + rnorm(10, 0, 10)\ndf &lt;- data.frame(x, y)\ndf\n\n    x             y\n1   1  5.323605e-01\n2   2  3.468994e+00\n3   3 -7.013884e+00\n4   4  3.167791e+01\n5   5  9.397093e+01\n6   6  1.413180e+02\n7   7  2.533070e+02\n8   8  3.813409e+02\n9   9  5.573736e+02\n10 10  7.862314e+02\n11 11  1.100532e+03\n12 12  1.453469e+03\n13 13  1.852986e+03\n14 14  2.361678e+03\n15 15  2.953971e+03\n16 16  3.591318e+03\n17 17  4.353307e+03\n18 18  5.191341e+03\n19 19  6.137374e+03\n20 20  7.196231e+03\n21 21  8.400532e+03\n22 22  9.703469e+03\n23 23  1.111299e+04\n24 24  1.269168e+04\n25 25  1.441397e+04\n26 26  1.624132e+04\n27 27  1.825331e+04\n28 28  2.040134e+04\n29 29  2.271737e+04\n30 30  2.520623e+04\n31 31  2.790053e+04\n32 32  3.075347e+04\n33 33  3.377299e+04\n34 34  3.702168e+04\n35 35  4.047397e+04\n36 36  4.409132e+04\n37 37  4.795331e+04\n38 38  5.201134e+04\n39 39  5.629737e+04\n40 40  6.081623e+04\n41 41  6.560053e+04\n42 42  7.060347e+04\n43 43  7.583299e+04\n44 44  8.135168e+04\n45 45  8.713397e+04\n46 46  9.314132e+04\n47 47  9.945331e+04\n48 48  1.060213e+05\n49 49  1.128774e+05\n50 50  1.200262e+05\n\n\nOn fitting both polynomial (degree = 3) and linear models we can see that the former performs much better.\n\nfit &lt;- lm(y ~ x, data = df)\ndf |&gt; ggplot(aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x) + \n  geom_smooth(method = 'loess', formula = y~poly(x, 3))\n\n\n\n\n\n\nConclusion\nIn summary, linear regression excels in capturing straightforward relationships between variables, offering simplicity and interpretability. Meanwhile, non-linear regression shines when dealing with more complex, non-linear data patterns that linear models cannot effectively represent.\nThe selection between linear and non-linear regression hinges on the nature of the data and the complexity of the relationship between variables. While linear regression provides a clear interpretation of linear trends, non-linear regression accommodates more nuanced, intricate relationships.\nBoth techniques possess their unique strengths and limitations. Linear regression is straightforward and computationally less demanding, whereas non-linear regression offers flexibility at the cost of potentially higher computational resources and the need to guard against overfitting.\nA comprehensive understanding of both linear and non-linear regression equips analysts and researchers with a versatile toolkit to effectively model relationships within diverse datasets. This proficiency aids in making accurate predictions and informed decisions across various domains, optimizing analyses for specific data complexities and objectives."
  },
  {
    "objectID": "posts/Anomaly detection/index.html#conclusion",
    "href": "posts/Anomaly detection/index.html#conclusion",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Anomaly and outlier detection play a pivotal role in data analysis across diverse domains. By leveraging statistical, machine learning, and visualization techniques, analysts can uncover hidden insights, potential errors, or significant events within datasets.\nUnderstanding the context and domain-specific knowledge are essential in defining anomalies and selecting suitable detection methods. Robust anomaly detection not only aids in identifying issues but also reveals valuable insights that might lead to improved decision-making, anomaly prevention, and enhanced overall data quality. Mastering anomaly detection techniques empowers analysts to extract meaningful information and glean deeper insights from their data, fostering more informed actions across various industries and disciplines."
  },
  {
    "objectID": "posts/classification/index.html#types-of-classification-algorithms",
    "href": "posts/classification/index.html#types-of-classification-algorithms",
    "title": "Classification",
    "section": "",
    "text": "Numerous algorithms cater to classification tasks, each with its strengths and suitability for different scenarios. Some of the widely used ones include:\n\n\nLogistic regression estimates the probability (ranging from 0 to 1) of an instance belonging to a specific class, unlike linear regression that predicts continuous values. Achieving this probability estimation involves using the logistic function (sigmoid function).\nTo classify instances, logistic regression applies a threshold to these probability scores. For instance, if the probability exceeds 0.5, it assigns the instance to one class; otherwise, it assigns it to the other, establishing a decision boundary.\nIn modeling the relationship between features and the logarithm of event odds, logistic regression assigns coefficients to each feature. These coefficients signify the impact of features on the outcome, aiding in understanding feature importance during classification. We’ll utilize a dataset containing details about patients susceptible to or not susceptible to strokes. Specifically, we’re employing the Stroke Prediction Dataset obtained from Kaggle to conduct predictions. Our objective involves analyzing the patient data within the training set to forecast whether a patient within the evaluation set is prone to experiencing a stroke or not.\n\ndf_stroke&lt;-read.csv(\"healthcare-dataset-stroke-data.csv\")\nglimpse(df_stroke)\n\nRows: 5,110\nColumns: 12\n$ id                &lt;int&gt; 9046, 51676, 31112, 60182, 1665, 56669, 53882, 10434…\n$ gender            &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Female\", \"Female\", \"Male\"…\n$ age               &lt;dbl&gt; 67, 61, 80, 49, 79, 81, 74, 69, 59, 78, 81, 61, 54, …\n$ hypertension      &lt;int&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1…\n$ heart_disease     &lt;int&gt; 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0…\n$ ever_married      &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No…\n$ work_type         &lt;chr&gt; \"Private\", \"Self-employed\", \"Private\", \"Private\", \"S…\n$ Residence_type    &lt;chr&gt; \"Urban\", \"Rural\", \"Rural\", \"Urban\", \"Rural\", \"Urban\"…\n$ avg_glucose_level &lt;dbl&gt; 228.69, 202.21, 105.92, 171.23, 174.12, 186.21, 70.0…\n$ bmi               &lt;chr&gt; \"36.6\", \"N/A\", \"32.5\", \"34.4\", \"24\", \"29\", \"27.4\", \"…\n$ smoking_status    &lt;chr&gt; \"formerly smoked\", \"never smoked\", \"never smoked\", \"…\n$ stroke            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nOur data glimpse shows that we have 5110 observations and 12 variables.\nWe can generate several bar graphs to visualize the connections between each of these factors and the target variable, which is the likelihood of a stroke occurrence in the individual.\n\np1 &lt;- ggplot(data = df_stroke) +geom_bar(mapping = aes(x = gender,fill=stroke))\np2 &lt;-ggplot(data = df_stroke) +geom_bar(mapping = aes(x = hypertension,fill=stroke))\np3 &lt;-ggplot(data = df_stroke) +geom_bar(mapping = aes(x = heart_disease,fill=stroke)) \np4 &lt;-ggplot(data = df_stroke) +geom_bar(mapping = aes(x = ever_married,fill=stroke)) \ngrid.arrange(p1,p2,p3,p4 ,ncol= 2)\n\n\n\np5 &lt;- ggplot(data = df_stroke) +geom_bar(mapping = aes(x = work_type,fill=stroke))\np6 &lt;-ggplot(data = df_stroke) +geom_bar(mapping = aes(x = Residence_type,fill=stroke))\np7 &lt;-ggplot(data = df_stroke) +geom_bar(mapping = aes(x = smoking_status,fill=stroke)) \ngrid.arrange(p5,p6,p7 ,ncol= 1)\n\n\n\n\nNow we prepare the training and test sets from the data.\n\ndf_stroke &lt;- na.omit(df_stroke)\nn_obs &lt;- nrow(df_stroke)\nsplit &lt;- round(n_obs * 0.7)\ntrain &lt;- df_stroke[1:split,]\n# Create test\ntest &lt;- df_stroke[(split + 1):nrow(df_stroke),]\n\n\nmodel &lt;- logistic_reg(mixture = double(1), penalty = double(1)) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"classification\") %&gt;%\n  fit(stroke ~ ., data = train)\n\n#confusionMatrix(factor(as.list(predict(model, test, type = \"class\"))), factor(test$stroke))\n\n\n\n\nDecision trees in classification start by selecting the best attribute at each node to divide the data, using measures like Gini impurity or information gain. This recursive process continues until the data is segregated into distinct subsets or meets certain stopping conditions, such as a specified tree depth or a minimum number of samples in a node.\nIn the classification process, new data instances navigate through the tree based on their attribute values. As they traverse the branches, they eventually arrive at a leaf node, where the decision tree assigns a predicted class label based on the attributes of the instance.\nOne of the advantages of decision trees is their ability to handle both categorical and numerical data. For categorical attributes, decision trees consider all possible outcomes for splitting, while for numerical attributes, the trees choose specific thresholds to partition the data efficiently.\n\n\n\nAn ensemble method comprising multiple decision trees, resulting in robust and accurate predictions.\nTo set up the model, we call the randomForest classifier and point it to ‘stroke’ column for the outcome and provide the ‘train’ set as input.\n\nrf_model&lt;-randomForest(formula= stroke~.,data = train)\n\nAfter training the model using the training set, it’s essential to assess its performance on comparable data. To achieve this, we’ll utilize the test dataset. Let’s display the confusion matrix to analyze how effectively our classification model worked when applied to the test data.\n\nconfusionMatrix(predict(rf_model, test), test$stroke)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   No  Yes\n       No  1469    0\n       Yes    3    0\n                                          \n               Accuracy : 0.998           \n                 95% CI : (0.9941, 0.9996)\n    No Information Rate : 1               \n    P-Value [Acc &gt; NIR] : 1.0000          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : 0.2482          \n                                          \n            Sensitivity : 0.998           \n            Specificity :    NA           \n         Pos Pred Value :    NA           \n         Neg Pred Value :    NA           \n             Prevalence : 1.000           \n         Detection Rate : 0.998           \n   Detection Prevalence : 0.998           \n      Balanced Accuracy :    NA           \n                                          \n       'Positive' Class : No              \n                                          \n\n\n\n\n\nSVM operates by identifying the hyperplane that maximizes the margin, which is the distance between the hyperplane and the nearest data points of each class, also known as support vectors. This hyperplane, while linear in nature for the basic implementation, aims to create the largest separation between classes, providing robustness in classifying unseen data.\nIn its standard form, SVM performs linear classification. However, by using kernel functions like polynomial, radial basis function (RBF), or sigmoid, SVMs can efficiently handle non-linear relationships by transforming the input data into higher-dimensional spaces where the classes become linearly separable.\n\nclassifier = svm(formula = stroke ~ ., \n                 data = train, \n                 type = 'C-classification', \n                 kernel = 'linear') \n\nconfusionMatrix(predict(classifier, test), test$stroke)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   No  Yes\n       No  1472    0\n       Yes    0    0\n                                     \n               Accuracy : 1          \n                 95% CI : (0.9975, 1)\n    No Information Rate : 1          \n    P-Value [Acc &gt; NIR] : 1          \n                                     \n                  Kappa : NaN        \n                                     \n Mcnemar's Test P-Value : NA         \n                                     \n            Sensitivity :  1         \n            Specificity : NA         \n         Pos Pred Value : NA         \n         Neg Pred Value : NA         \n             Prevalence :  1         \n         Detection Rate :  1         \n   Detection Prevalence :  1         \n      Balanced Accuracy : NA         \n                                     \n       'Positive' Class : No         \n                                     \n\n\n\n\n\nNeural networks process data using layers of neurons. The input layer receives data, hidden layers process information, and the output layer produces predictions.\nActivation functions add non-linearities to neurons, enabling the model to learn complex data patterns and relationships.\nThrough training, neural networks adjust internal parameters (weights and biases) by comparing predicted outputs with actual labels using optimization algorithms like gradient descent.\nDeep neural networks, with multiple hidden layers, excel in capturing intricate data features and hierarchies, making them well-suited for handling complex classification tasks."
  },
  {
    "objectID": "posts/classification/index.html#conclusion",
    "href": "posts/classification/index.html#conclusion",
    "title": "Classification",
    "section": "",
    "text": "In conclusion, machine learning classification offers diverse tools like SVM, Decision Trees, and Neural Networks, vital for informed decision-making across industries. Understanding these methods and their strengths is crucial for effective use. Challenges like overfitting and data imbalance require careful handling. Mastering these techniques empowers better insights, automation, and innovation in various domains, marking a fundamental aspect of data-driven problem-solving."
  }
]